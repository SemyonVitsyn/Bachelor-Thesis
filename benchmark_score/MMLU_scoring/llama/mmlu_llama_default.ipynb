{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa558e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `LLaMa` has been saved to /home/user/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/user/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `LLaMa`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afabdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e63968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 22:58:56.068865: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746734336.087634  164137 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746734336.093392  164137 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746734336.109105  164137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746734336.109119  164137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746734336.109122  164137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746734336.109124  164137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-08 22:58:56.114983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS_TOKEN=<|begin_of_text|>\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/110 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   1%|          | 1/110 [00:07<13:57,  7.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   2%|▏         | 2/110 [00:16<14:34,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   3%|▎         | 3/110 [00:25<15:32,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   4%|▎         | 4/110 [00:34<15:44,  8.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   5%|▍         | 5/110 [00:39<13:17,  7.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   5%|▌         | 6/110 [00:46<12:44,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   6%|▋         | 7/110 [00:55<13:21,  7.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   7%|▋         | 8/110 [01:04<14:04,  8.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   8%|▊         | 9/110 [01:16<15:38,  9.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   9%|▉         | 10/110 [01:31<18:23, 11.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  10%|█         | 11/110 [01:46<20:06, 12.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  11%|█         | 12/110 [01:56<18:56, 11.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  12%|█▏        | 13/110 [02:04<16:50, 10.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  13%|█▎        | 14/110 [02:09<14:05,  8.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  14%|█▎        | 15/110 [02:18<14:15,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  15%|█▍        | 16/110 [02:23<12:23,  7.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  15%|█▌        | 17/110 [02:32<12:21,  7.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  16%|█▋        | 18/110 [02:40<12:11,  7.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  17%|█▋        | 19/110 [02:48<12:23,  8.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  18%|█▊        | 20/110 [03:00<13:48,  9.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  19%|█▉        | 21/110 [03:09<13:31,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  20%|██        | 22/110 [03:18<13:29,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  21%|██        | 23/110 [03:28<13:38,  9.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  22%|██▏       | 24/110 [03:38<13:48,  9.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  23%|██▎       | 25/110 [03:49<14:05,  9.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  24%|██▎       | 26/110 [04:04<15:58, 11.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  25%|██▍       | 27/110 [04:19<17:16, 12.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  25%|██▌       | 28/110 [04:24<14:19, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  26%|██▋       | 29/110 [04:31<12:24,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  27%|██▋       | 30/110 [04:38<11:38,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  28%|██▊       | 31/110 [04:46<11:00,  8.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  29%|██▉       | 32/110 [04:55<11:00,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  30%|███       | 33/110 [05:02<10:31,  8.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  31%|███       | 34/110 [05:10<10:27,  8.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  32%|███▏      | 35/110 [05:19<10:17,  8.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  33%|███▎      | 36/110 [05:27<10:21,  8.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  34%|███▎      | 37/110 [05:38<10:51,  8.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  35%|███▍      | 38/110 [05:50<11:56,  9.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  35%|███▌      | 39/110 [06:03<12:50, 10.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  36%|███▋      | 40/110 [06:11<11:39,  9.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  37%|███▋      | 41/110 [06:22<11:52, 10.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  38%|███▊      | 42/110 [06:33<11:57, 10.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  39%|███▉      | 43/110 [06:46<12:26, 11.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  40%|████      | 44/110 [07:00<13:26, 12.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  41%|████      | 45/110 [07:15<14:08, 13.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  42%|████▏     | 46/110 [07:30<14:33, 13.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  43%|████▎     | 47/110 [07:45<14:45, 14.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  44%|████▎     | 48/110 [08:00<14:47, 14.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  45%|████▍     | 49/110 [08:06<11:47, 11.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  45%|████▌     | 50/110 [08:14<10:35, 10.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  46%|████▋     | 51/110 [08:24<10:14, 10.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  47%|████▋     | 52/110 [08:33<09:43, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  48%|████▊     | 53/110 [08:41<09:04,  9.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  49%|████▉     | 54/110 [08:50<08:45,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  50%|█████     | 55/110 [09:00<08:43,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  51%|█████     | 56/110 [09:07<07:41,  8.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  52%|█████▏    | 57/110 [09:13<07:00,  7.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  53%|█████▎    | 58/110 [09:19<06:18,  7.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  54%|█████▎    | 59/110 [09:27<06:21,  7.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  55%|█████▍    | 60/110 [09:34<06:18,  7.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  55%|█████▌    | 61/110 [09:40<05:36,  6.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  56%|█████▋    | 62/110 [09:49<06:00,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  57%|█████▋    | 63/110 [09:58<06:17,  8.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  58%|█████▊    | 64/110 [10:05<05:52,  7.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  59%|█████▉    | 65/110 [10:13<05:48,  7.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  60%|██████    | 66/110 [10:20<05:40,  7.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  61%|██████    | 67/110 [10:28<05:27,  7.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  62%|██████▏   | 68/110 [10:35<05:15,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  63%|██████▎   | 69/110 [10:43<05:08,  7.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  64%|██████▎   | 70/110 [10:50<04:54,  7.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  65%|██████▍   | 71/110 [10:57<04:52,  7.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  65%|██████▌   | 72/110 [11:04<04:39,  7.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  66%|██████▋   | 73/110 [11:12<04:31,  7.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  67%|██████▋   | 74/110 [11:20<04:31,  7.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  68%|██████▊   | 75/110 [11:29<04:40,  8.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  69%|██████▉   | 76/110 [11:37<04:32,  8.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  70%|███████   | 77/110 [11:47<04:46,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  71%|███████   | 78/110 [11:58<04:54,  9.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  72%|███████▏  | 79/110 [12:06<04:39,  9.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  73%|███████▎  | 80/110 [12:13<04:13,  8.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  74%|███████▎  | 81/110 [12:23<04:14,  8.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  75%|███████▍  | 82/110 [12:34<04:27,  9.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  75%|███████▌  | 83/110 [12:45<04:27,  9.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  76%|███████▋  | 84/110 [13:00<04:57, 11.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  77%|███████▋  | 85/110 [13:15<05:12, 12.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  78%|███████▊  | 86/110 [13:30<05:18, 13.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  79%|███████▉  | 87/110 [13:45<05:16, 13.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  80%|████████  | 88/110 [14:00<05:11, 14.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  81%|████████  | 89/110 [14:15<05:02, 14.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  82%|████████▏ | 90/110 [14:30<04:51, 14.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  83%|████████▎ | 91/110 [14:45<04:39, 14.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  84%|████████▎ | 92/110 [15:00<04:26, 14.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  85%|████████▍ | 93/110 [15:15<04:12, 14.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  85%|████████▌ | 94/110 [15:30<03:58, 14.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  86%|████████▋ | 95/110 [15:45<03:43, 14.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  87%|████████▋ | 96/110 [16:00<03:29, 14.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  88%|████████▊ | 97/110 [16:15<03:13, 14.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  89%|████████▉ | 98/110 [16:26<02:47, 13.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  90%|█████████ | 99/110 [16:36<02:20, 12.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  91%|█████████ | 100/110 [16:49<02:05, 12.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  92%|█████████▏| 101/110 [17:03<01:59, 13.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  93%|█████████▎| 102/110 [17:14<01:39, 12.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  94%|█████████▎| 103/110 [17:29<01:32, 13.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  95%|█████████▍| 104/110 [17:44<01:22, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  95%|█████████▌| 105/110 [17:59<01:10, 14.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  96%|█████████▋| 106/110 [18:05<00:47, 11.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  97%|█████████▋| 107/110 [18:13<00:31, 10.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  98%|█████████▊| 108/110 [18:20<00:19,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  99%|█████████▉| 109/110 [18:35<00:11, 11.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches: 100%|██████████| 110/110 [18:39<00:00, 10.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Test Results\n",
      "========================================\n",
      "Total Accuracy: 21.96%\n",
      "\n",
      "Accuracy by Subject:\n",
      "subject_en\n",
      "world_religions                       32.16%\n",
      "human_aging                           31.84%\n",
      "machine_learning                      31.25%\n",
      "business_ethics                       31.00%\n",
      "medical_genetics                      30.00%\n",
      "formal_logic                          28.57%\n",
      "virology                              28.31%\n",
      "marketing                             28.21%\n",
      "us_foreign_policy                     28.00%\n",
      "human_sexuality                       26.72%\n",
      "conceptual_physics                    26.38%\n",
      "computer_security                     26.00%\n",
      "jurisprudence                         25.93%\n",
      "professional_psychology               25.16%\n",
      "college_biology                       25.00%\n",
      "high_school_computer_science          25.00%\n",
      "moral_disputes                        24.86%\n",
      "econometrics                          24.56%\n",
      "college_computer_science              24.00%\n",
      "international_law                     23.97%\n",
      "professional_accounting               23.76%\n",
      "miscellaneous                         23.63%\n",
      "moral_scenarios                       23.58%\n",
      "electrical_engineering                23.45%\n",
      "sociology                             23.38%\n",
      "nutrition                             22.55%\n",
      "logical_fallacies                     22.09%\n",
      "abstract_algebra                      22.00%\n",
      "prehistory                            21.91%\n",
      "public_relations                      21.82%\n",
      "college_physics                       21.57%\n",
      "clinical_knowledge                    21.51%\n",
      "high_school_mathematics               21.11%\n",
      "high_school_microeconomics            21.01%\n",
      "college_mathematics                   21.00%\n",
      "elementary_mathematics                20.90%\n",
      "high_school_macroeconomics            20.51%\n",
      "professional_law                      20.21%\n",
      "college_chemistry                     20.00%\n",
      "high_school_physics                   19.87%\n",
      "high_school_psychology                19.82%\n",
      "high_school_government_and_politics   19.69%\n",
      "college_medicine                      19.65%\n",
      "security_studies                      19.59%\n",
      "philosophy                            18.65%\n",
      "anatomy                               18.52%\n",
      "professional_medicine                 18.38%\n",
      "high_school_geography                 18.18%\n",
      "high_school_biology                   18.06%\n",
      "global_facts                          18.00%\n",
      "astronomy                             17.76%\n",
      "management                            17.48%\n",
      "high_school_chemistry                 15.27%\n",
      "high_school_statistics                14.81%\n",
      "high_school_world_history             14.77%\n",
      "high_school_us_history                12.75%\n",
      "high_school_european_history          12.12%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "MODEL_NAME = \"llama-1b-raw\"\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# MODEL_PATH = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# SYSTEM = [{\"role\": \"system\", \"content\": \"\"}]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "BOS_TOKEN = tokenizer.bos_token if tokenizer.bos_token else tokenizer.additional_special_tokens[0] \n",
    "print(f\"BOS_TOKEN={BOS_TOKEN}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,  # Путь к объединенной модели\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# subjects = [\"high_school_biology\"]\n",
    "\n",
    "subjects = [\"abstract_algebra\", \"anatomy\", \"astronomy\", \"business_ethics\", \"clinical_knowledge\",\n",
    "             \"college_biology\", \"college_chemistry\", \"college_computer_science\", \"college_mathematics\",\n",
    "               \"college_medicine\", \"college_physics\", \"computer_security\", \"conceptual_physics\", \"econometrics\",\n",
    "                 \"electrical_engineering\", \"elementary_mathematics\", \"formal_logic\", \"global_facts\",\n",
    "                   \"high_school_biology\", \"high_school_chemistry\", \"high_school_computer_science\",\n",
    "                     \"high_school_european_history\", \"high_school_geography\", \"high_school_government_and_politics\",\n",
    "                       \"high_school_macroeconomics\", \"high_school_mathematics\", \"high_school_microeconomics\",\n",
    "                         \"high_school_physics\", \"high_school_psychology\", \"high_school_statistics\", \"high_school_us_history\",\n",
    "                           \"high_school_world_history\", \"human_aging\", \"human_sexuality\", \"international_law\",\n",
    "                             \"jurisprudence\", \"logical_fallacies\", \"machine_learning\", \"management\", \"marketing\",\n",
    "                               \"medical_genetics\", \"miscellaneous\", \"moral_disputes\", \"moral_scenarios\", \"nutrition\",\n",
    "                                 \"philosophy\", \"prehistory\", \"professional_accounting\", \"professional_law\",\n",
    "                                   \"professional_medicine\", \"professional_psychology\", \"public_relations\",\n",
    "                                     \"security_studies\", \"sociology\", \"us_foreign_policy\", \"virology\", \"world_religions\"]\n",
    "\n",
    "\n",
    "all_datasets = {subject: datasets.load_dataset(\"NLPCoreTeam/mmlu_ru\", name=subject, split=\"test\") for subject in subjects}\n",
    "\n",
    "test_dfs = []\n",
    "for subject in subjects:\n",
    "    dataset = all_datasets[subject]\n",
    "    df = dataset.to_pandas()\n",
    "    int2str = dataset.features['answer'].int2str\n",
    "    df['answer'] = df['answer'].map(int2str)\n",
    "    df.insert(0, 'subject_en', subject)\n",
    "    test_dfs.append(df)\n",
    "\n",
    "test_df = pd.concat(test_dfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_prompt(row):\n",
    "    return (\n",
    "        f\"Дан вопрос по теме {row['subject_en']}: {row['question_ru']}. Варианты ответа:\\n\"\n",
    "        f\"A) {row['choices_ru'][0]}\\nB) {row['choices_ru'][1]}\\nC) {row['choices_ru'][2]}\\nD) {row['choices_ru'][3]}\\n\"\n",
    "        \"Твой ответ должен быть в формате 'Ответ: <Буква>'.\\n\"\n",
    "        \"Закончи ответ, указав только одну букву: A, B, C или D.\\n\"\n",
    "    )\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [\n",
    "        {\"role\": \"user\", \"content\": create_prompt(row)},\n",
    "        # {\"role\": \"assistant\", \"content\": \"aboba\"},\n",
    "    ]\n",
    "    return formatted_message\n",
    "\n",
    "def extract_answer(text):\n",
    "    text = text.upper().strip()\n",
    "\n",
    "    explicit_pattern = re.search(\n",
    "        r\"(?:Ответ|ANSWER|Правильный ответ|Answer)[\\s:\\-—]*([A-D])\", \n",
    "        text\n",
    "    )\n",
    "    if explicit_pattern:\n",
    "        return explicit_pattern.group(1)\n",
    "\n",
    "    for char in text:\n",
    "        if char in {'A','B','C','D'}:\n",
    "            return char\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def evaluate_test(df, model, tokenizer):\n",
    "    device = model.device\n",
    "    df['prediction'] = ''\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch = df.iloc[i:i+BATCH_SIZE]\n",
    "        prompts = [generate_conversation(row) for _, row in batch.iterrows()]\n",
    "        # print(chat_prompts[0])\n",
    "\n",
    "        chat_prompts = tokenizer.apply_chat_template(\n",
    "            prompts,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # print(chat_prompts[0])\n",
    "        # break\n",
    "        # chat_prompts = [prompt + f\"{tokenizer.convert_ids_to_tokens(128006)}assistant{tokenizer.convert_ids_to_tokens(128007)}\\n\" for prompt in chat_prompts]\n",
    "        # print(chat_prompts[0])\n",
    "        # break\n",
    "        # print(chat_prompts)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            chat_prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                top_k=None,\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.input_ids.shape[1]:], \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        \n",
    "        for j, text in enumerate(decoded):\n",
    "            # print(f\"---------------------------------------------------\\n{text.strip()}\\n---------------------------------------------------\")\n",
    "            answer = extract_answer(text)\n",
    "            df.at[i+j, 'prediction'] = answer\n",
    "            \n",
    "        # break\n",
    "\n",
    "    df['correct'] = df['answer'] == df['prediction']\n",
    "    total_acc = df['correct'].mean()\n",
    "    subject_acc = df.groupby('subject_en')['correct'].mean()\n",
    "\n",
    "\n",
    "    return total_acc, subject_acc\n",
    "\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "total_accuracy, subject_accuracy = evaluate_test(test_df, model, tokenizer)\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*40}\\nTest Results\\n{'='*40}\")\n",
    "print(f\"Total Accuracy: {total_accuracy:.2%}\")\n",
    "print(\"\\nAccuracy by Subject:\")\n",
    "print(subject_accuracy.sort_values(ascending=False).to_string(float_format=\"{:,.2%}\".format))\n",
    "\n",
    "\n",
    "test_df.to_csv(f\"../result/mmlu_{MODEL_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2d8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e397a967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|start_header_id|>', '<|end_header_id|>')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(128006), tokenizer.convert_ids_to_tokens(128007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc5b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02054838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
