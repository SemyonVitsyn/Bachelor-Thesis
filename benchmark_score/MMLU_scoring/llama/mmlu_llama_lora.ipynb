{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be84eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "The token `LLaMa` has been saved to /home/user/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/user/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `LLaMa`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dbda1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 23:20:13.678510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746735613.699570  164565 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746735613.705610  164565 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746735613.721033  164565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746735613.721050  164565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746735613.721052  164565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746735613.721054  164565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-08 23:20:13.726438: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS_TOKEN=<|begin_of_text|>\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/110 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   1%|          | 1/110 [00:08<14:42,  8.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   2%|▏         | 2/110 [00:16<14:56,  8.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   3%|▎         | 3/110 [00:26<16:03,  9.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   4%|▎         | 4/110 [00:35<16:05,  9.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   5%|▍         | 5/110 [00:41<13:51,  7.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   5%|▌         | 6/110 [00:49<13:50,  7.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   6%|▋         | 7/110 [01:00<15:25,  8.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   7%|▋         | 8/110 [01:10<15:29,  9.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   8%|▊         | 9/110 [01:21<16:38,  9.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   9%|▉         | 10/110 [01:36<19:06, 11.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  10%|█         | 11/110 [01:51<20:34, 12.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  11%|█         | 12/110 [02:01<19:18, 11.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  12%|█▏        | 13/110 [02:09<17:06, 10.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  13%|█▎        | 14/110 [02:14<14:27,  9.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  14%|█▎        | 15/110 [02:24<14:32,  9.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  15%|█▍        | 16/110 [02:29<12:36,  8.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  15%|█▌        | 17/110 [02:37<12:31,  8.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  16%|█▋        | 18/110 [02:45<12:19,  8.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  17%|█▋        | 19/110 [02:54<12:31,  8.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  18%|█▊        | 20/110 [03:06<13:56,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  19%|█▉        | 21/110 [03:15<13:39,  9.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  20%|██        | 22/110 [03:24<13:30,  9.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  21%|██        | 23/110 [03:34<13:42,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  22%|██▏       | 24/110 [03:44<13:53,  9.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  23%|██▎       | 25/110 [03:55<14:11, 10.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  24%|██▎       | 26/110 [04:10<16:06, 11.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  25%|██▍       | 27/110 [04:25<17:25, 12.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  25%|██▌       | 28/110 [04:33<15:09, 11.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  26%|██▋       | 29/110 [04:41<13:45, 10.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  27%|██▋       | 30/110 [04:51<13:25, 10.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  28%|██▊       | 31/110 [04:58<12:17,  9.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  29%|██▉       | 32/110 [05:07<11:55,  9.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  30%|███       | 33/110 [05:16<11:33,  9.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  31%|███       | 34/110 [05:24<11:10,  8.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  32%|███▏      | 35/110 [05:32<10:50,  8.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  33%|███▎      | 36/110 [05:41<10:47,  8.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  34%|███▎      | 37/110 [05:51<11:02,  9.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  35%|███▍      | 38/110 [06:04<12:05, 10.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  35%|███▌      | 39/110 [06:17<13:01, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  36%|███▋      | 40/110 [06:26<12:05, 10.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  37%|███▋      | 41/110 [06:37<12:11, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  38%|███▊      | 42/110 [06:48<12:11, 10.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  39%|███▉      | 43/110 [07:01<12:37, 11.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  40%|████      | 44/110 [07:15<13:36, 12.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  41%|████      | 45/110 [07:31<14:16, 13.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  42%|████▏     | 46/110 [07:46<14:40, 13.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  43%|████▎     | 47/110 [08:01<14:52, 14.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  44%|████▎     | 48/110 [08:16<14:53, 14.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  45%|████▍     | 49/110 [08:22<12:10, 11.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  45%|████▌     | 50/110 [08:29<10:31, 10.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  46%|████▋     | 51/110 [08:39<10:12, 10.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  47%|████▋     | 52/110 [08:48<09:43, 10.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  48%|████▊     | 53/110 [08:57<09:04,  9.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  49%|████▉     | 54/110 [09:06<08:47,  9.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  50%|█████     | 55/110 [09:16<08:45,  9.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  51%|█████     | 56/110 [09:24<08:13,  9.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  52%|█████▏    | 57/110 [09:31<07:36,  8.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  53%|█████▎    | 58/110 [09:37<06:47,  7.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  54%|█████▎    | 59/110 [09:46<06:47,  7.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  55%|█████▍    | 60/110 [09:54<06:47,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  55%|█████▌    | 61/110 [10:00<06:07,  7.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  56%|█████▋    | 62/110 [10:12<06:57,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  57%|█████▋    | 63/110 [10:23<07:22,  9.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  58%|█████▊    | 64/110 [10:30<06:38,  8.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  59%|█████▉    | 65/110 [10:38<06:28,  8.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  60%|██████    | 66/110 [10:47<06:17,  8.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  61%|██████    | 67/110 [10:55<05:59,  8.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  62%|██████▏   | 68/110 [11:01<05:26,  7.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  63%|██████▎   | 69/110 [11:08<05:03,  7.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  64%|██████▎   | 70/110 [11:14<04:39,  7.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  65%|██████▍   | 71/110 [11:20<04:30,  6.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  65%|██████▌   | 72/110 [11:26<04:13,  6.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  66%|██████▋   | 73/110 [11:33<04:05,  6.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  67%|██████▋   | 74/110 [11:42<04:24,  7.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  68%|██████▊   | 75/110 [11:51<04:37,  7.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  69%|██████▉   | 76/110 [12:01<04:44,  8.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  70%|███████   | 77/110 [12:10<04:42,  8.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  71%|███████   | 78/110 [12:20<04:48,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  72%|███████▏  | 79/110 [12:29<04:42,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  73%|███████▎  | 80/110 [12:38<04:33,  9.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  74%|███████▎  | 81/110 [12:49<04:36,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  75%|███████▍  | 82/110 [13:00<04:41, 10.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  75%|███████▌  | 83/110 [13:11<04:37, 10.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  76%|███████▋  | 84/110 [13:26<05:04, 11.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  77%|███████▋  | 85/110 [13:41<05:17, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  78%|███████▊  | 86/110 [13:56<05:21, 13.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  79%|███████▉  | 87/110 [14:11<05:19, 13.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  80%|████████  | 88/110 [14:26<05:13, 14.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  81%|████████  | 89/110 [14:41<05:04, 14.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  82%|████████▏ | 90/110 [14:56<04:52, 14.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  83%|████████▎ | 91/110 [15:11<04:40, 14.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  84%|████████▎ | 92/110 [15:26<04:27, 14.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  85%|████████▍ | 93/110 [15:41<04:13, 14.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  85%|████████▌ | 94/110 [15:56<03:59, 14.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  86%|████████▋ | 95/110 [16:11<03:44, 14.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  87%|████████▋ | 96/110 [16:26<03:29, 14.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  88%|████████▊ | 97/110 [16:41<03:14, 14.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  89%|████████▉ | 98/110 [16:53<02:47, 13.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  90%|█████████ | 99/110 [17:02<02:19, 12.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  91%|█████████ | 100/110 [17:15<02:05, 12.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  92%|█████████▏| 101/110 [17:29<01:58, 13.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  93%|█████████▎| 102/110 [17:40<01:39, 12.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  94%|█████████▎| 103/110 [17:55<01:32, 13.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  95%|█████████▍| 104/110 [18:10<01:22, 13.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  95%|█████████▌| 105/110 [18:25<01:10, 14.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  96%|█████████▋| 106/110 [18:34<00:49, 12.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  97%|█████████▋| 107/110 [18:41<00:33, 11.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  98%|█████████▊| 108/110 [18:51<00:21, 10.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  99%|█████████▉| 109/110 [19:06<00:11, 11.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches: 100%|██████████| 110/110 [19:10<00:00, 10.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Test Results\n",
      "========================================\n",
      "Total Accuracy: 32.55%\n",
      "\n",
      "Accuracy by Subject:\n",
      "subject_en\n",
      "us_foreign_policy                     59.00%\n",
      "international_law                     57.85%\n",
      "marketing                             49.57%\n",
      "jurisprudence                         45.37%\n",
      "sociology                             44.78%\n",
      "security_studies                      44.08%\n",
      "philosophy                            43.09%\n",
      "nutrition                             42.81%\n",
      "astronomy                             42.76%\n",
      "human_sexuality                       42.75%\n",
      "electrical_engineering                42.07%\n",
      "public_relations                      41.82%\n",
      "high_school_computer_science          41.00%\n",
      "business_ethics                       41.00%\n",
      "high_school_biology                   40.32%\n",
      "virology                              39.76%\n",
      "anatomy                               39.26%\n",
      "clinical_knowledge                    38.11%\n",
      "computer_security                     38.00%\n",
      "miscellaneous                         37.68%\n",
      "human_aging                           37.67%\n",
      "logical_fallacies                     37.42%\n",
      "high_school_geography                 36.36%\n",
      "prehistory                            36.11%\n",
      "moral_disputes                        35.55%\n",
      "professional_psychology               34.97%\n",
      "high_school_psychology                34.50%\n",
      "machine_learning                      33.93%\n",
      "high_school_macroeconomics            33.33%\n",
      "high_school_chemistry                 33.00%\n",
      "world_religions                       32.75%\n",
      "management                            32.04%\n",
      "abstract_algebra                      32.00%\n",
      "college_computer_science              32.00%\n",
      "high_school_physics                   31.79%\n",
      "high_school_microeconomics            31.51%\n",
      "elementary_mathematics                30.95%\n",
      "medical_genetics                      30.00%\n",
      "college_biology                       29.86%\n",
      "high_school_government_and_politics   29.53%\n",
      "college_physics                       29.41%\n",
      "college_mathematics                   29.00%\n",
      "conceptual_physics                    28.94%\n",
      "global_facts                          27.00%\n",
      "high_school_european_history          26.67%\n",
      "professional_accounting               25.89%\n",
      "high_school_statistics                25.46%\n",
      "college_medicine                      25.43%\n",
      "moral_scenarios                       24.92%\n",
      "formal_logic                          24.60%\n",
      "econometrics                          23.68%\n",
      "professional_law                      22.49%\n",
      "college_chemistry                     22.00%\n",
      "professional_medicine                 20.96%\n",
      "high_school_mathematics               20.00%\n",
      "high_school_world_history             18.57%\n",
      "high_school_us_history                17.65%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "MODEL_NAME = \"llama-1b-lora\"\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# MODEL_PATH = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# SYSTEM = [{\"role\": \"system\", \"content\": \"\"}]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "BOS_TOKEN = tokenizer.bos_token if tokenizer.bos_token else tokenizer.additional_special_tokens[0] \n",
    "print(f\"BOS_TOKEN={BOS_TOKEN}\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_PATH,  # Путь к объединенной модели\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../../llama-v100-bs_12_2/merged_model\",  # Путь к объединенной модели\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# subjects = [\"high_school_biology\"]\n",
    "\n",
    "subjects = [\"abstract_algebra\", \"anatomy\", \"astronomy\", \"business_ethics\", \"clinical_knowledge\",\n",
    "             \"college_biology\", \"college_chemistry\", \"college_computer_science\", \"college_mathematics\",\n",
    "               \"college_medicine\", \"college_physics\", \"computer_security\", \"conceptual_physics\", \"econometrics\",\n",
    "                 \"electrical_engineering\", \"elementary_mathematics\", \"formal_logic\", \"global_facts\",\n",
    "                   \"high_school_biology\", \"high_school_chemistry\", \"high_school_computer_science\",\n",
    "                     \"high_school_european_history\", \"high_school_geography\", \"high_school_government_and_politics\",\n",
    "                       \"high_school_macroeconomics\", \"high_school_mathematics\", \"high_school_microeconomics\",\n",
    "                         \"high_school_physics\", \"high_school_psychology\", \"high_school_statistics\", \"high_school_us_history\",\n",
    "                           \"high_school_world_history\", \"human_aging\", \"human_sexuality\", \"international_law\",\n",
    "                             \"jurisprudence\", \"logical_fallacies\", \"machine_learning\", \"management\", \"marketing\",\n",
    "                               \"medical_genetics\", \"miscellaneous\", \"moral_disputes\", \"moral_scenarios\", \"nutrition\",\n",
    "                                 \"philosophy\", \"prehistory\", \"professional_accounting\", \"professional_law\",\n",
    "                                   \"professional_medicine\", \"professional_psychology\", \"public_relations\",\n",
    "                                     \"security_studies\", \"sociology\", \"us_foreign_policy\", \"virology\", \"world_religions\"]\n",
    "\n",
    "\n",
    "all_datasets = {subject: datasets.load_dataset(\"NLPCoreTeam/mmlu_ru\", name=subject, split=\"test\") for subject in subjects}\n",
    "\n",
    "test_dfs = []\n",
    "for subject in subjects:\n",
    "    dataset = all_datasets[subject]\n",
    "    df = dataset.to_pandas()\n",
    "    int2str = dataset.features['answer'].int2str\n",
    "    df['answer'] = df['answer'].map(int2str)\n",
    "    df.insert(0, 'subject_en', subject)\n",
    "    test_dfs.append(df)\n",
    "\n",
    "test_df = pd.concat(test_dfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_prompt(row):\n",
    "    return (\n",
    "        f\"Дан вопрос по теме {row['subject_en']}: {row['question_ru']}. Варианты ответа:\\n\"\n",
    "        f\"A) {row['choices_ru'][0]}\\nB) {row['choices_ru'][1]}\\nC) {row['choices_ru'][2]}\\nD) {row['choices_ru'][3]}\\n\"\n",
    "        \"Твой ответ должен быть в формате 'Ответ: <Буква>'.\\n\"\n",
    "        \"Закончи ответ, указав только одну букву: A, B, C или D.\\n\"\n",
    "    )\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [\n",
    "        {\"role\": \"user\", \"content\": create_prompt(row)},\n",
    "        # {\"role\": \"assistant\", \"content\": \"aboba\"},\n",
    "    ]\n",
    "    return formatted_message\n",
    "\n",
    "def extract_answer(text):\n",
    "    text = text.upper().strip()\n",
    "\n",
    "    explicit_pattern = re.search(\n",
    "        r\"(?:Ответ|ANSWER|Правильный ответ|Answer)[\\s:\\-—]*([A-D])\", \n",
    "        text\n",
    "    )\n",
    "    if explicit_pattern:\n",
    "        return explicit_pattern.group(1)\n",
    "\n",
    "    for char in text:\n",
    "        if char in {'A','B','C','D'}:\n",
    "            return char\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def evaluate_test(df, model, tokenizer):\n",
    "    device = model.device\n",
    "    df['prediction'] = ''\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch = df.iloc[i:i+BATCH_SIZE]\n",
    "        prompts = [generate_conversation(row) for _, row in batch.iterrows()]\n",
    "        # print(chat_prompts[0])\n",
    "\n",
    "        chat_prompts = tokenizer.apply_chat_template(\n",
    "            prompts,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # print(chat_prompts[0])\n",
    "        # break\n",
    "        # chat_prompts = [prompt + f\"{tokenizer.convert_ids_to_tokens(128006)}assistant{tokenizer.convert_ids_to_tokens(128007)}\\n\" for prompt in chat_prompts]\n",
    "        # print(chat_prompts[0])\n",
    "        # break\n",
    "        # print(chat_prompts)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            chat_prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                top_k=None,\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.input_ids.shape[1]:], \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        \n",
    "        for j, text in enumerate(decoded):\n",
    "            # print(f\"---------------------------------------------------\\n{text.strip()}\\n---------------------------------------------------\")\n",
    "            answer = extract_answer(text)\n",
    "            df.at[i+j, 'prediction'] = answer\n",
    "            \n",
    "        # break\n",
    "\n",
    "    df['correct'] = df['answer'] == df['prediction']\n",
    "    total_acc = df['correct'].mean()\n",
    "    subject_acc = df.groupby('subject_en')['correct'].mean()\n",
    "\n",
    "\n",
    "    return total_acc, subject_acc\n",
    "\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "total_accuracy, subject_accuracy = evaluate_test(test_df, model, tokenizer)\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*40}\\nTest Results\\n{'='*40}\")\n",
    "print(f\"Total Accuracy: {total_accuracy:.2%}\")\n",
    "print(\"\\nAccuracy by Subject:\")\n",
    "print(subject_accuracy.sort_values(ascending=False).to_string(float_format=\"{:,.2%}\".format))\n",
    "\n",
    "\n",
    "test_df.to_csv(f\"../result/mmlu_{MODEL_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656b400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
