{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be84eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `LLaMa` has been saved to /home/user/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/user/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `LLaMa`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dbda1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:13:21.391860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746814401.410492  100811 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746814401.416167  100811 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746814401.432728  100811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746814401.432743  100811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746814401.432746  100811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746814401.432747  100811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-09 21:13:21.438503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS_TOKEN=<|begin_of_text|>\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/110 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   1%|          | 1/110 [00:08<14:43,  8.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   2%|▏         | 2/110 [00:17<15:48,  8.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   3%|▎         | 3/110 [00:27<16:31,  9.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   4%|▎         | 4/110 [00:36<16:21,  9.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   5%|▍         | 5/110 [00:42<14:16,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   5%|▌         | 6/110 [00:51<14:35,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   6%|▋         | 7/110 [01:02<15:54,  9.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   7%|▋         | 8/110 [01:12<15:50,  9.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   8%|▊         | 9/110 [01:23<16:52, 10.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   9%|▉         | 10/110 [01:38<19:13, 11.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  10%|█         | 11/110 [01:53<20:40, 12.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  11%|█         | 12/110 [02:03<19:22, 11.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  12%|█▏        | 13/110 [02:11<17:09, 10.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  13%|█▎        | 14/110 [02:17<14:48,  9.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  14%|█▎        | 15/110 [02:27<14:46,  9.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  15%|█▍        | 16/110 [02:33<13:05,  8.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  15%|█▌        | 17/110 [02:41<12:51,  8.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  16%|█▋        | 18/110 [02:49<12:32,  8.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  17%|█▋        | 19/110 [02:57<12:40,  8.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  18%|█▊        | 20/110 [03:09<14:01,  9.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  19%|█▉        | 21/110 [03:18<13:41,  9.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  20%|██        | 22/110 [03:27<13:37,  9.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  21%|██        | 23/110 [03:37<13:44,  9.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  22%|██▏       | 24/110 [03:48<13:53,  9.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  23%|██▎       | 25/110 [03:58<14:10, 10.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  24%|██▎       | 26/110 [04:13<16:04, 11.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  25%|██▍       | 27/110 [04:28<17:21, 12.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  25%|██▌       | 28/110 [04:36<15:07, 11.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  26%|██▋       | 29/110 [04:44<13:43, 10.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  27%|██▋       | 30/110 [04:54<13:25, 10.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  28%|██▊       | 31/110 [05:02<12:36,  9.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  29%|██▉       | 32/110 [05:11<12:08,  9.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  30%|███       | 33/110 [05:20<11:42,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  31%|███       | 34/110 [05:28<11:16,  8.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  32%|███▏      | 35/110 [05:36<10:54,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  33%|███▎      | 36/110 [05:45<10:49,  8.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  34%|███▎      | 37/110 [05:55<11:10,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  35%|███▍      | 38/110 [06:08<12:11, 10.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  35%|███▌      | 39/110 [06:21<13:04, 11.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  36%|███▋      | 40/110 [06:30<12:18, 10.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  37%|███▋      | 41/110 [06:41<12:20, 10.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  38%|███▊      | 42/110 [06:53<12:18, 10.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  39%|███▉      | 43/110 [07:05<12:41, 11.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  40%|████      | 44/110 [07:20<13:40, 12.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  41%|████      | 45/110 [07:35<14:18, 13.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  42%|████▏     | 46/110 [07:50<14:40, 13.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  43%|████▎     | 47/110 [08:05<14:52, 14.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  44%|████▎     | 48/110 [08:20<14:53, 14.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  45%|████▍     | 49/110 [08:27<12:15, 12.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  45%|████▌     | 50/110 [08:34<10:43, 10.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  46%|████▋     | 51/110 [08:44<10:20, 10.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  47%|████▋     | 52/110 [08:54<09:49, 10.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  48%|████▊     | 53/110 [09:02<09:08,  9.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  49%|████▉     | 54/110 [09:11<08:49,  9.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  50%|█████     | 55/110 [09:21<08:46,  9.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  51%|█████     | 56/110 [09:29<08:14,  9.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  52%|█████▏    | 57/110 [09:37<07:40,  8.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  53%|█████▎    | 58/110 [09:44<07:00,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  54%|█████▎    | 59/110 [09:53<07:13,  8.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  55%|█████▍    | 60/110 [10:02<07:16,  8.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  55%|█████▌    | 61/110 [10:09<06:42,  8.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  56%|█████▋    | 62/110 [10:21<07:20,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  57%|█████▋    | 63/110 [10:32<07:38,  9.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  58%|█████▊    | 64/110 [10:39<06:59,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  59%|█████▉    | 65/110 [10:48<06:41,  8.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  60%|██████    | 66/110 [10:57<06:31,  8.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  61%|██████    | 67/110 [11:05<06:16,  8.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  62%|██████▏   | 68/110 [11:12<05:44,  8.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  63%|██████▎   | 69/110 [11:19<05:22,  7.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  64%|██████▎   | 70/110 [11:26<04:59,  7.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  65%|██████▍   | 71/110 [11:35<05:08,  7.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  65%|██████▌   | 72/110 [11:41<04:45,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  66%|██████▋   | 73/110 [11:48<04:32,  7.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  67%|██████▋   | 74/110 [11:57<04:43,  7.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  68%|██████▊   | 75/110 [12:07<04:55,  8.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  69%|██████▉   | 76/110 [12:17<04:57,  8.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  70%|███████   | 77/110 [12:27<05:04,  9.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  71%|███████   | 78/110 [12:38<05:14,  9.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  72%|███████▏  | 79/110 [12:48<05:00,  9.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  73%|███████▎  | 80/110 [12:57<04:45,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  74%|███████▎  | 81/110 [13:07<04:43,  9.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  75%|███████▍  | 82/110 [13:18<04:46, 10.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  75%|███████▌  | 83/110 [13:29<04:41, 10.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  76%|███████▋  | 84/110 [13:44<05:06, 11.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  77%|███████▋  | 85/110 [13:59<05:19, 12.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  78%|███████▊  | 86/110 [14:14<05:22, 13.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  79%|███████▉  | 87/110 [14:29<05:19, 13.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  80%|████████  | 88/110 [14:44<05:13, 14.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  81%|████████  | 89/110 [14:59<05:03, 14.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  82%|████████▏ | 90/110 [15:14<04:52, 14.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  83%|████████▎ | 91/110 [15:29<04:40, 14.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  84%|████████▎ | 92/110 [15:44<04:26, 14.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  85%|████████▍ | 93/110 [15:59<04:13, 14.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  85%|████████▌ | 94/110 [16:14<03:58, 14.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  86%|████████▋ | 95/110 [16:29<03:44, 14.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  87%|████████▋ | 96/110 [16:44<03:29, 14.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  88%|████████▊ | 97/110 [16:59<03:14, 14.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  89%|████████▉ | 98/110 [17:11<02:48, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  90%|█████████ | 99/110 [17:21<02:21, 12.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  91%|█████████ | 100/110 [17:33<02:06, 12.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  92%|█████████▏| 101/110 [17:48<01:59, 13.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  93%|█████████▎| 102/110 [17:59<01:40, 12.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  94%|█████████▎| 103/110 [18:14<01:32, 13.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  95%|█████████▍| 104/110 [18:29<01:22, 13.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  95%|█████████▌| 105/110 [18:44<01:10, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  96%|█████████▋| 106/110 [18:52<00:49, 12.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  97%|█████████▋| 107/110 [19:00<00:33, 11.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  98%|█████████▊| 108/110 [19:10<00:21, 10.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  99%|█████████▉| 109/110 [19:25<00:11, 11.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches: 100%|██████████| 110/110 [19:29<00:00, 10.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Test Results\n",
      "========================================\n",
      "Total Accuracy: 30.33%\n",
      "\n",
      "Accuracy by Subject:\n",
      "subject_en\n",
      "international_law                     52.89%\n",
      "us_foreign_policy                     49.00%\n",
      "marketing                             48.29%\n",
      "sociology                             45.27%\n",
      "philosophy                            44.05%\n",
      "electrical_engineering                42.76%\n",
      "anatomy                               42.22%\n",
      "public_relations                      40.91%\n",
      "jurisprudence                         40.74%\n",
      "astronomy                             39.47%\n",
      "human_sexuality                       38.93%\n",
      "computer_security                     38.00%\n",
      "high_school_computer_science          38.00%\n",
      "high_school_biology                   37.42%\n",
      "clinical_knowledge                    35.47%\n",
      "human_aging                           35.43%\n",
      "prehistory                            35.19%\n",
      "business_ethics                       35.00%\n",
      "high_school_chemistry                 34.98%\n",
      "moral_disputes                        34.39%\n",
      "nutrition                             34.31%\n",
      "logical_fallacies                     33.74%\n",
      "security_studies                      33.47%\n",
      "abstract_algebra                      33.00%\n",
      "high_school_geography                 32.83%\n",
      "miscellaneous                         32.57%\n",
      "virology                              32.53%\n",
      "professional_psychology               32.52%\n",
      "high_school_psychology                31.93%\n",
      "college_computer_science              31.00%\n",
      "high_school_microeconomics            30.25%\n",
      "elementary_mathematics                29.63%\n",
      "world_religions                       29.24%\n",
      "management                            29.13%\n",
      "college_mathematics                   29.00%\n",
      "professional_accounting               28.72%\n",
      "conceptual_physics                    28.51%\n",
      "high_school_government_and_politics   28.50%\n",
      "college_medicine                      27.75%\n",
      "high_school_macroeconomics            27.44%\n",
      "college_chemistry                     26.00%\n",
      "medical_genetics                      25.00%\n",
      "moral_scenarios                       24.58%\n",
      "econometrics                          24.56%\n",
      "college_physics                       24.51%\n",
      "college_biology                       24.31%\n",
      "professional_medicine                 24.26%\n",
      "high_school_european_history          24.24%\n",
      "global_facts                          24.00%\n",
      "high_school_statistics                23.61%\n",
      "high_school_physics                   22.52%\n",
      "professional_law                      21.45%\n",
      "formal_logic                          21.43%\n",
      "machine_learning                      20.54%\n",
      "high_school_mathematics               18.89%\n",
      "high_school_world_history             17.30%\n",
      "high_school_us_history                16.18%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "MODEL_NAME = \"llama-1b-webglm\"\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# SYSTEM = [{\"role\": \"system\", \"content\": \"\"}]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side='left')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "BOS_TOKEN = tokenizer.bos_token if tokenizer.bos_token else tokenizer.additional_special_tokens[0] \n",
    "print(f\"BOS_TOKEN={BOS_TOKEN}\")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../../llama-v100-bs_12_2-webglm_ft/merged_model\",  # Путь к объединенной модели\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# subjects = [\"high_school_biology\"]\n",
    "\n",
    "subjects = [\"abstract_algebra\", \"anatomy\", \"astronomy\", \"business_ethics\", \"clinical_knowledge\",\n",
    "             \"college_biology\", \"college_chemistry\", \"college_computer_science\", \"college_mathematics\",\n",
    "               \"college_medicine\", \"college_physics\", \"computer_security\", \"conceptual_physics\", \"econometrics\",\n",
    "                 \"electrical_engineering\", \"elementary_mathematics\", \"formal_logic\", \"global_facts\",\n",
    "                   \"high_school_biology\", \"high_school_chemistry\", \"high_school_computer_science\",\n",
    "                     \"high_school_european_history\", \"high_school_geography\", \"high_school_government_and_politics\",\n",
    "                       \"high_school_macroeconomics\", \"high_school_mathematics\", \"high_school_microeconomics\",\n",
    "                         \"high_school_physics\", \"high_school_psychology\", \"high_school_statistics\", \"high_school_us_history\",\n",
    "                           \"high_school_world_history\", \"human_aging\", \"human_sexuality\", \"international_law\",\n",
    "                             \"jurisprudence\", \"logical_fallacies\", \"machine_learning\", \"management\", \"marketing\",\n",
    "                               \"medical_genetics\", \"miscellaneous\", \"moral_disputes\", \"moral_scenarios\", \"nutrition\",\n",
    "                                 \"philosophy\", \"prehistory\", \"professional_accounting\", \"professional_law\",\n",
    "                                   \"professional_medicine\", \"professional_psychology\", \"public_relations\",\n",
    "                                     \"security_studies\", \"sociology\", \"us_foreign_policy\", \"virology\", \"world_religions\"]\n",
    "\n",
    "\n",
    "all_datasets = {subject: datasets.load_dataset(\"NLPCoreTeam/mmlu_ru\", name=subject, split=\"test\") for subject in subjects}\n",
    "\n",
    "test_dfs = []\n",
    "for subject in subjects:\n",
    "    dataset = all_datasets[subject]\n",
    "    df = dataset.to_pandas()\n",
    "    int2str = dataset.features['answer'].int2str\n",
    "    df['answer'] = df['answer'].map(int2str)\n",
    "    df.insert(0, 'subject_en', subject)\n",
    "    test_dfs.append(df)\n",
    "\n",
    "test_df = pd.concat(test_dfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_prompt(row):\n",
    "    return (\n",
    "        f\"Дан вопрос по теме {row['subject_en']}: {row['question_ru']}. Варианты ответа:\\n\"\n",
    "        f\"A) {row['choices_ru'][0]}\\nB) {row['choices_ru'][1]}\\nC) {row['choices_ru'][2]}\\nD) {row['choices_ru'][3]}\\n\"\n",
    "        \"Твой ответ должен быть в формате 'Ответ: <Буква>'.\\n\"\n",
    "        \"Закончи ответ, указав только одну букву: A, B, C или D.\\n\"\n",
    "    )\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [\n",
    "        {\"role\": \"user\", \"content\": create_prompt(row)},\n",
    "        # {\"role\": \"assistant\", \"content\": \"aboba\"},\n",
    "    ]\n",
    "    return formatted_message\n",
    "\n",
    "def extract_answer(text):\n",
    "    text = text.upper().strip()\n",
    "\n",
    "    explicit_pattern = re.search(\n",
    "        r\"(?:Ответ|ANSWER|Правильный ответ|Answer)[\\s:\\-—]*([A-D])\", \n",
    "        text\n",
    "    )\n",
    "    if explicit_pattern:\n",
    "        return explicit_pattern.group(1)\n",
    "\n",
    "    for char in text:\n",
    "        if char in {'A','B','C','D'}:\n",
    "            return char\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def evaluate_test(df, model, tokenizer):\n",
    "    device = model.device\n",
    "    df['prediction'] = ''\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch = df.iloc[i:i+BATCH_SIZE]\n",
    "        prompts = [generate_conversation(row) for _, row in batch.iterrows()]\n",
    "        # print(chat_prompts[0])\n",
    "\n",
    "        chat_prompts = tokenizer.apply_chat_template(\n",
    "            prompts,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # print(chat_prompts[0])\n",
    "        # break\n",
    "        # chat_prompts = [prompt + f\"{tokenizer.convert_ids_to_tokens(128006)}assistant{tokenizer.convert_ids_to_tokens(128007)}\\n\" for prompt in chat_prompts]\n",
    "        # print(chat_prompts[0])\n",
    "        # break\n",
    "        # print(chat_prompts)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            chat_prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                top_k=None,\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.input_ids.shape[1]:], \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        \n",
    "        for j, text in enumerate(decoded):\n",
    "            # print(f\"---------------------------------------------------\\n{text.strip()}\\n---------------------------------------------------\")\n",
    "            answer = extract_answer(text)\n",
    "            df.at[i+j, 'prediction'] = answer\n",
    "            \n",
    "        # break\n",
    "\n",
    "    df['correct'] = df['answer'] == df['prediction']\n",
    "    total_acc = df['correct'].mean()\n",
    "    subject_acc = df.groupby('subject_en')['correct'].mean()\n",
    "\n",
    "\n",
    "    return total_acc, subject_acc\n",
    "\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "total_accuracy, subject_accuracy = evaluate_test(test_df, model, tokenizer)\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*40}\\nTest Results\\n{'='*40}\")\n",
    "print(f\"Total Accuracy: {total_accuracy:.2%}\")\n",
    "print(\"\\nAccuracy by Subject:\")\n",
    "print(subject_accuracy.sort_values(ascending=False).to_string(float_format=\"{:,.2%}\".format))\n",
    "\n",
    "\n",
    "test_df.to_csv(f\"../result/mmlu_{MODEL_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f656b400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
