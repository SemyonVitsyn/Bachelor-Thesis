{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11143022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 13:19:26.951764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746785966.973639    7766 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746785966.980207    7766 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746785966.997696    7766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746785966.997715    7766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746785966.997718    7766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746785966.997721    7766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-09 13:19:27.003999: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS_TOKEN=<|begin_of_text|>\n",
      "Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/110 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   1%|          | 1/110 [00:06<11:11,  6.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   2%|▏         | 2/110 [00:13<11:54,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   3%|▎         | 3/110 [00:20<12:04,  6.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   4%|▎         | 4/110 [00:24<10:21,  5.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   5%|▍         | 5/110 [00:30<10:01,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   5%|▌         | 6/110 [00:37<10:40,  6.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   6%|▋         | 7/110 [00:44<11:26,  6.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   7%|▋         | 8/110 [00:52<12:10,  7.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   8%|▊         | 9/110 [01:01<12:36,  7.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:   9%|▉         | 10/110 [01:16<16:19,  9.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  10%|█         | 11/110 [01:30<18:38, 11.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  11%|█         | 12/110 [01:38<16:53, 10.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  12%|█▏        | 13/110 [01:44<14:28,  8.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  13%|█▎        | 14/110 [01:49<12:24,  7.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  14%|█▎        | 15/110 [01:57<12:10,  7.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  15%|█▍        | 16/110 [02:02<10:48,  6.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  15%|█▌        | 17/110 [02:08<10:22,  6.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  16%|█▋        | 18/110 [02:15<10:15,  6.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  17%|█▋        | 19/110 [02:21<10:11,  6.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  18%|█▊        | 20/110 [02:30<10:56,  7.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  19%|█▉        | 21/110 [02:36<10:24,  7.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  20%|██        | 22/110 [02:43<10:16,  7.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  21%|██        | 23/110 [02:51<10:18,  7.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  22%|██▏       | 24/110 [02:58<10:22,  7.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  23%|██▎       | 25/110 [03:07<10:42,  7.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  24%|██▎       | 26/110 [03:22<13:39,  9.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  25%|██▍       | 27/110 [03:36<15:36, 11.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  25%|██▌       | 28/110 [03:41<12:34,  9.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  26%|██▋       | 29/110 [03:45<10:28,  7.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  27%|██▋       | 30/110 [03:52<10:00,  7.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  28%|██▊       | 31/110 [03:58<09:19,  7.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  29%|██▉       | 32/110 [04:04<08:49,  6.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  30%|███       | 33/110 [04:10<08:20,  6.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  31%|███       | 34/110 [04:17<08:19,  6.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  32%|███▏      | 35/110 [04:24<08:23,  6.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  33%|███▎      | 36/110 [04:31<08:27,  6.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  34%|███▎      | 37/110 [04:38<08:20,  6.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  35%|███▍      | 38/110 [04:47<09:04,  7.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  35%|███▌      | 39/110 [04:56<09:26,  7.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  36%|███▋      | 40/110 [05:02<08:42,  7.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  37%|███▋      | 41/110 [05:10<08:42,  7.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  38%|███▊      | 42/110 [05:18<08:32,  7.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  39%|███▉      | 43/110 [05:26<08:43,  7.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  40%|████      | 44/110 [05:38<09:48,  8.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  41%|████      | 45/110 [05:52<11:36, 10.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  42%|████▏     | 46/110 [06:07<12:46, 11.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  43%|████▎     | 47/110 [06:22<13:29, 12.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  44%|████▎     | 48/110 [06:37<13:53, 13.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  45%|████▍     | 49/110 [06:41<10:39, 10.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  45%|████▌     | 50/110 [06:45<08:35,  8.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  46%|████▋     | 51/110 [06:50<07:32,  7.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  47%|████▋     | 52/110 [06:57<07:11,  7.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  48%|████▊     | 53/110 [07:02<06:13,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  49%|████▉     | 54/110 [07:09<06:10,  6.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  50%|█████     | 55/110 [07:17<06:36,  7.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  51%|█████     | 56/110 [07:23<06:01,  6.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  52%|█████▏    | 57/110 [07:28<05:32,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  53%|█████▎    | 58/110 [07:33<05:09,  5.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  54%|█████▎    | 59/110 [07:40<05:24,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  55%|█████▍    | 60/110 [07:48<05:30,  6.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  55%|█████▌    | 61/110 [07:53<05:06,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  56%|█████▋    | 62/110 [08:02<05:41,  7.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  57%|█████▋    | 63/110 [08:11<05:51,  7.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  58%|█████▊    | 64/110 [08:17<05:25,  7.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  59%|█████▉    | 65/110 [08:21<04:46,  6.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  60%|██████    | 66/110 [08:26<04:18,  5.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  61%|██████    | 67/110 [08:31<03:54,  5.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  62%|██████▏   | 68/110 [08:35<03:36,  5.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  63%|██████▎   | 69/110 [08:39<03:22,  4.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  64%|██████▎   | 70/110 [08:44<03:09,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  65%|██████▍   | 71/110 [08:48<03:02,  4.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  65%|██████▌   | 72/110 [08:53<02:53,  4.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  66%|██████▋   | 73/110 [08:57<02:46,  4.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  67%|██████▋   | 74/110 [09:02<02:43,  4.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  68%|██████▊   | 75/110 [09:08<02:59,  5.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  69%|██████▉   | 76/110 [09:14<03:07,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  70%|███████   | 77/110 [09:20<03:02,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  71%|███████   | 78/110 [09:26<03:01,  5.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  72%|███████▏  | 79/110 [09:31<02:52,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  73%|███████▎  | 80/110 [09:36<02:41,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  74%|███████▎  | 81/110 [09:44<02:53,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  75%|███████▍  | 82/110 [09:51<03:02,  6.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  75%|███████▌  | 83/110 [09:59<03:05,  6.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  76%|███████▋  | 84/110 [10:14<04:00,  9.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  77%|███████▋  | 85/110 [10:29<04:32, 10.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  78%|███████▊  | 86/110 [10:44<04:50, 12.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  79%|███████▉  | 87/110 [10:58<04:56, 12.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  80%|████████  | 88/110 [11:13<04:56, 13.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  81%|████████  | 89/110 [11:28<04:51, 13.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  82%|████████▏ | 90/110 [11:43<04:43, 14.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  83%|████████▎ | 91/110 [11:58<04:32, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  84%|████████▎ | 92/110 [12:12<04:20, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  85%|████████▍ | 93/110 [12:27<04:07, 14.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  85%|████████▌ | 94/110 [12:42<03:54, 14.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  86%|████████▋ | 95/110 [12:57<03:40, 14.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  87%|████████▋ | 96/110 [13:09<03:16, 14.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  88%|████████▊ | 97/110 [13:21<02:55, 13.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  89%|████████▉ | 98/110 [13:30<02:24, 12.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  90%|█████████ | 99/110 [13:38<01:57, 10.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  91%|█████████ | 100/110 [13:46<01:40, 10.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  92%|█████████▏| 101/110 [13:55<01:27,  9.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  93%|█████████▎| 102/110 [14:01<01:08,  8.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  94%|█████████▎| 103/110 [14:09<00:59,  8.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  95%|█████████▍| 104/110 [14:19<00:53,  8.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  95%|█████████▌| 105/110 [14:27<00:43,  8.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  96%|█████████▋| 106/110 [14:32<00:30,  7.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  97%|█████████▋| 107/110 [14:36<00:19,  6.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  98%|█████████▊| 108/110 [14:41<00:11,  5.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches:  99%|█████████▉| 109/110 [14:52<00:07,  7.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Processing batches: 100%|██████████| 110/110 [14:54<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Test Results\n",
      "========================================\n",
      "Total Accuracy: 40.81%\n",
      "\n",
      "Accuracy by Subject:\n",
      "subject_en\n",
      "us_foreign_policy                     69.00%\n",
      "marketing                             64.96%\n",
      "international_law                     61.98%\n",
      "miscellaneous                         57.85%\n",
      "sociology                             57.71%\n",
      "world_religions                       56.73%\n",
      "computer_security                     56.00%\n",
      "human_aging                           54.26%\n",
      "high_school_psychology                53.58%\n",
      "high_school_geography                 53.54%\n",
      "management                            53.40%\n",
      "human_sexuality                       52.67%\n",
      "security_studies                      51.02%\n",
      "high_school_us_history                50.00%\n",
      "high_school_world_history             49.37%\n",
      "prehistory                            48.15%\n",
      "high_school_government_and_politics   47.67%\n",
      "jurisprudence                         47.22%\n",
      "college_biology                       47.22%\n",
      "virology                              46.99%\n",
      "electrical_engineering                46.90%\n",
      "clinical_knowledge                    46.79%\n",
      "high_school_biology                   46.77%\n",
      "nutrition                             46.73%\n",
      "anatomy                               46.67%\n",
      "medical_genetics                      45.00%\n",
      "moral_disputes                        44.22%\n",
      "astronomy                             44.08%\n",
      "high_school_european_history          43.64%\n",
      "philosophy                            43.41%\n",
      "professional_medicine                 43.38%\n",
      "high_school_computer_science          43.00%\n",
      "logical_fallacies                     42.33%\n",
      "public_relations                      40.91%\n",
      "professional_psychology               40.20%\n",
      "business_ethics                       40.00%\n",
      "conceptual_physics                    38.30%\n",
      "college_medicine                      38.15%\n",
      "professional_law                      33.05%\n",
      "high_school_chemistry                 33.00%\n",
      "high_school_microeconomics            32.35%\n",
      "machine_learning                      30.36%\n",
      "high_school_macroeconomics            30.00%\n",
      "formal_logic                          29.37%\n",
      "professional_accounting               29.08%\n",
      "econometrics                          28.07%\n",
      "college_chemistry                     27.00%\n",
      "global_facts                          26.00%\n",
      "high_school_physics                   25.83%\n",
      "moral_scenarios                       25.25%\n",
      "college_computer_science              25.00%\n",
      "elementary_mathematics                24.34%\n",
      "college_physics                       23.53%\n",
      "abstract_algebra                      22.00%\n",
      "high_school_mathematics               21.48%\n",
      "college_mathematics                   20.00%\n",
      "high_school_statistics                17.13%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "MODEL_NAME = \"llama-1b-raw-eng\"\n",
    "MODEL_PATH = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# MODEL_PATH = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# SYSTEM = [{\"role\": \"system\", \"content\": \"\"}]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "BOS_TOKEN = tokenizer.bos_token if tokenizer.bos_token else tokenizer.additional_special_tokens[0] \n",
    "print(f\"BOS_TOKEN={BOS_TOKEN}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,  # Путь к объединенной модели\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# subjects = [\"high_school_biology\"]\n",
    "\n",
    "subjects = [\"abstract_algebra\", \"anatomy\", \"astronomy\", \"business_ethics\", \"clinical_knowledge\",\n",
    "             \"college_biology\", \"college_chemistry\", \"college_computer_science\", \"college_mathematics\",\n",
    "               \"college_medicine\", \"college_physics\", \"computer_security\", \"conceptual_physics\", \"econometrics\",\n",
    "                 \"electrical_engineering\", \"elementary_mathematics\", \"formal_logic\", \"global_facts\",\n",
    "                   \"high_school_biology\", \"high_school_chemistry\", \"high_school_computer_science\",\n",
    "                     \"high_school_european_history\", \"high_school_geography\", \"high_school_government_and_politics\",\n",
    "                       \"high_school_macroeconomics\", \"high_school_mathematics\", \"high_school_microeconomics\",\n",
    "                         \"high_school_physics\", \"high_school_psychology\", \"high_school_statistics\", \"high_school_us_history\",\n",
    "                           \"high_school_world_history\", \"human_aging\", \"human_sexuality\", \"international_law\",\n",
    "                             \"jurisprudence\", \"logical_fallacies\", \"machine_learning\", \"management\", \"marketing\",\n",
    "                               \"medical_genetics\", \"miscellaneous\", \"moral_disputes\", \"moral_scenarios\", \"nutrition\",\n",
    "                                 \"philosophy\", \"prehistory\", \"professional_accounting\", \"professional_law\",\n",
    "                                   \"professional_medicine\", \"professional_psychology\", \"public_relations\",\n",
    "                                     \"security_studies\", \"sociology\", \"us_foreign_policy\", \"virology\", \"world_religions\"]\n",
    "\n",
    "\n",
    "all_datasets = {subject: datasets.load_dataset(\"NLPCoreTeam/mmlu_ru\", name=subject, split=\"test\") for subject in subjects}\n",
    "\n",
    "test_dfs = []\n",
    "for subject in subjects:\n",
    "    dataset = all_datasets[subject]\n",
    "    df = dataset.to_pandas()\n",
    "    int2str = dataset.features['answer'].int2str\n",
    "    df['answer'] = df['answer'].map(int2str)\n",
    "    df.insert(0, 'subject_en', subject)\n",
    "    test_dfs.append(df)\n",
    "\n",
    "test_df = pd.concat(test_dfs).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_prompt(row):\n",
    "    return (\n",
    "        f\"The question is on topic {row['subject_en']}: {row['question_en']}. Answer options:\\n\"\n",
    "        f\"A) {row['choices_en'][0]}\\nB) {row['choices_en'][1]}\\nC) {row['choices_en'][2]}\\nD) {row['choices_en'][3]}\\n\"\n",
    "        \"Your answer must be in the format 'Answer: <Letter>'.\\n\"\n",
    "        \"Complete the answer using only one letter: A, B, C, or D.\\n\"\n",
    "    )\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [\n",
    "        {\"role\": \"user\", \"content\": create_prompt(row)},\n",
    "        # {\"role\": \"assistant\", \"content\": \"aboba\"},\n",
    "    ]\n",
    "    return formatted_message\n",
    "\n",
    "def extract_answer(text):\n",
    "    text = text.upper().strip()\n",
    "\n",
    "    explicit_pattern = re.search(\n",
    "        r\"(?:Ответ|ANSWER|Правильный ответ|Answer)[\\s:\\-—]*([A-D])\", \n",
    "        text\n",
    "    )\n",
    "    if explicit_pattern:\n",
    "        return explicit_pattern.group(1)\n",
    "\n",
    "    for char in text:\n",
    "        if char in {'A','B','C','D'}:\n",
    "            return char\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def evaluate_test(df, model, tokenizer):\n",
    "    device = model.device\n",
    "    df['prediction'] = ''\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch = df.iloc[i:i+BATCH_SIZE]\n",
    "        prompts = [generate_conversation(row) for _, row in batch.iterrows()]\n",
    "        # print(chat_prompts[0])\n",
    "\n",
    "        chat_prompts = tokenizer.apply_chat_template(\n",
    "            prompts,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # print(chat_prompts[0])\n",
    "        # break\n",
    "        # chat_prompts = [prompt + f\"{tokenizer.convert_ids_to_tokens(128006)}assistant{tokenizer.convert_ids_to_tokens(128007)}\\n\" for prompt in chat_prompts]\n",
    "        # print(chat_prompts[0])\n",
    "        # break\n",
    "        # print(chat_prompts)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            chat_prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                top_k=None,\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.input_ids.shape[1]:], \n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        \n",
    "        for j, text in enumerate(decoded):\n",
    "            # print(f\"---------------------------------------------------\\n{text.strip()}\\n---------------------------------------------------\")\n",
    "            answer = extract_answer(text)\n",
    "            df.at[i+j, 'prediction'] = answer\n",
    "            \n",
    "        # break\n",
    "\n",
    "    df['correct'] = df['answer'] == df['prediction']\n",
    "    total_acc = df['correct'].mean()\n",
    "    subject_acc = df.groupby('subject_en')['correct'].mean()\n",
    "\n",
    "\n",
    "    return total_acc, subject_acc\n",
    "\n",
    "\n",
    "print(\"Starting evaluation...\")\n",
    "total_accuracy, subject_accuracy = evaluate_test(test_df, model, tokenizer)\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*40}\\nTest Results\\n{'='*40}\")\n",
    "print(f\"Total Accuracy: {total_accuracy:.2%}\")\n",
    "print(\"\\nAccuracy by Subject:\")\n",
    "print(subject_accuracy.sort_values(ascending=False).to_string(float_format=\"{:,.2%}\".format))\n",
    "\n",
    "\n",
    "test_df.to_csv(f\"../result/mmlu_{MODEL_NAME}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
