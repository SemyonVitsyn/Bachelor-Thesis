{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "889877ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from openai import OpenAI\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"qwen-3b-raft\"\n",
    "\n",
    "shuffle = \"\"\n",
    "\n",
    "type = \"large\"\n",
    "\n",
    "\n",
    "\n",
    "JUDGE_NAME = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_judge_prompt(question, relevant_context, reference_answer, model_answer):\n",
    "    messages = []\n",
    "\n",
    "    messages.append({\"role\": \"system\", \"content\": \"You are a helpful assistant that scores RAG answers against the ground truth. Always respond with a JSON object \"\n",
    "                                                \"containing a 'comment' (string), 'is_inappropriate_refusal' (0-1), 'score' (number 1-5), 'is_correct' (0-1) fields.\"})\n",
    "\n",
    "    judge_prompt = f\"\"\"\n",
    "    **Question**: {question}\n",
    "    **Relevant context**: {relevant_context}\n",
    "    **Reference Answer**: {reference_answer}\n",
    "    **Model Answer**: {model_answer}\n",
    "\n",
    "    **Evaluation Rules**:\n",
    "    1. Scoring Scale:\n",
    "    - 5: Good answer (factually correct, complete)\n",
    "    - 4: Mostly correct with minor issues\n",
    "    - 3: Partially correct but has inaccuracies\n",
    "    - 2: Mostly incorrect but contains relevant elements\n",
    "    - 1: Completely wrong or irrelevant\n",
    "\n",
    "    2. Binary Correctness:\n",
    "    - 1 (Correct): Answer conveys the same meaning as reference (exact wording not required)\n",
    "    - 0 (Incorrect): Meaning differs from reference or contains false information\n",
    "\n",
    "    3. Binary Refuse:\n",
    "    - 1 (Inappropriate refusal): The model answer is a refusal, while the reference contains a full answer\n",
    "    - 0 (A normal answer or an appropriate refusal)\n",
    "\n",
    "    4. Key Principles:\n",
    "    - Focus on whether the model answers the question asked, not on whether it exactly matches the reference\n",
    "    - There is no need to lower the score if the model's answer omits some facts that are insignificant to the question\n",
    "    - If an answer seems redundant in relation to the reference, but the information is contained in a relevant context, then there is no need to lower the score for this\n",
    "    - Ignore stylistic differences if core meaning is preserved\n",
    "    - Inappropriate refusal should always have score 2 out of 5!\n",
    "\n",
    "    Provide analysis in JSON format: {{\"comment\": \"...\", \"is_inappropriate_refusal\": 0|1, \"score\": 1-5, \"is_correct\": 0|1}}\"\"\"\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": judge_prompt})\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f0740668",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"NEBIUS_API_KEY\")\n",
    "\n",
    "# client = OpenAI(\n",
    "#     base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "#     api_key=API_KEY\n",
    "# )\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8afa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"comment\": \"The model answer does not accurately address the question about the percentage of difficult-to-extract oil in Russia and its locations. Instead, it provides unrelated information about the total reserves and types of oil in different regions, without mentioning the specific percentage of difficult-to-extract oil or its primary locations as stated in the reference answer.\", \"score\": 1, \"is_correct\": 0, \"is_inappropriate_refusal\": 0}\n"
     ]
    }
   ],
   "source": [
    "# with open(\"generation/qwen-1.5b-raw.json\", \"r\") as f:\n",
    "#     benchmark_data = json.load(f)\n",
    "#     data=benchmark_data[1]\n",
    "\n",
    "# messages = get_judge_prompt(\n",
    "#     data['table_data']['Вопрос'],\n",
    "#     data['table_data']['Отрывок из документа'],\n",
    "#     data['table_data']['Ответ'],\n",
    "#     data['model_answer']\n",
    "# )\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"deepseek-ai/DeepSeek-V3\",\n",
    "#     messages=messages, \n",
    "#     temperature=0.5,\n",
    "#     max_completion_tokens=512\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ea60573",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_llm(messages : str, model : str, semaphore : asyncio.Semaphore):\n",
    "    async with semaphore:\n",
    "        return await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages, \n",
    "            temperature=0.5,\n",
    "            max_completion_tokens=512\n",
    "        )\n",
    "    \n",
    "\n",
    "async def process_task(task_id, messages, model, semaphore):\n",
    "    try:\n",
    "        response = await run_llm(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            semaphore=semaphore,\n",
    "        )\n",
    "        return {\n",
    "            \"task_id\": task_id,\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"task_id\": task_id,\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"failed\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ab687f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM requests: 100%|██████████| 231/231 [00:25<00:00,  9.08it/s]\n"
     ]
    }
   ],
   "source": [
    "semaphore = asyncio.Semaphore(50)\n",
    "\n",
    "with open(f\"generation_short{shuffle}/{MODEL_NAME}_{type}.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "tasks = [process_task(\n",
    "    task_id=i,\n",
    "    messages=get_judge_prompt(data['table_data']['Вопрос'], data['table_data']['Отрывок из документа'], data['table_data']['Ответ'], data['model_answer']),\n",
    "    model=JUDGE_NAME,\n",
    "    semaphore=semaphore)\n",
    "    for i, data in enumerate(benchmark_data)\n",
    "]\n",
    "\n",
    "results = []\n",
    "with tqdm_asyncio(total=len(tasks), desc=\"Processing LLM requests\") as pbar:\n",
    "    for coro in asyncio.as_completed(tasks):\n",
    "        result = await coro\n",
    "        results.append(result)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1e9877ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort(key=lambda x: x['task_id'])\n",
    "\n",
    "for result in results:\n",
    "    if result['status'] == \"failed\":\n",
    "        print(result['task_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4648b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(content):\n",
    "    json_str = re.search(r'\\{.*?\\}', content, re.DOTALL)\n",
    "\n",
    "    result = json.loads(json_str.group(0))\n",
    "\n",
    "    # keys = ['comment', 'score']\n",
    "\n",
    "    keys = ['comment', 'is_inappropriate_refusal', 'score', 'is_correct']\n",
    "\n",
    "    judgment = {}\n",
    "    for key in keys:\n",
    "        if key in result:\n",
    "            judgment[key] = result[key]\n",
    "        else:\n",
    "            judgment[key] = None\n",
    "\n",
    "    return judgment\n",
    "\n",
    "\n",
    "for i, data in enumerate(benchmark_data):\n",
    "    data[\"judgment\"] = parse(results[i]['content'])\n",
    "\n",
    "with open(f\"scored_short{shuffle}/{MODEL_NAME}_{type}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b2ed6b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "BASELINE avg score: 4.125541125541125\n",
      "BASELINE avg accuracy: 0.7532467532467533\n"
     ]
    }
   ],
   "source": [
    "with open(f\"scored_short{shuffle}/{MODEL_NAME}_{type}.json\", \"r\") as baseline:\n",
    "    baseline = json.load(baseline)\n",
    "\n",
    "    avg_baseline = 0\n",
    "    avg_accuracy = 0\n",
    "\n",
    "    for i, item in enumerate(baseline):\n",
    "        # print(\"-----------------------------------------------------------------\\n\")\n",
    "        # print(f\"Question: {raw[i]['table_data']['Вопрос']}\\n\")\n",
    "        # print(f\"Relevant context: {raw[i]['table_data']['Отрывок из документа']}\\n\")\n",
    "        # print(f\"Reference answer: {raw[i]['table_data']['Ответ']}\\n\")\n",
    "        # print(f\"RAW answer: {raw[i]['model_answer']}\\n\")\n",
    "        # print(f\"RAW judgment: {raw[i]['judgment']}\\n\")\n",
    "        # print(f\"BASELINE answer: {baseline[i]['model_answer']}\\n\")\n",
    "        # print(f\"BASELINE judgment: {baseline[i]['judgment']}\\n\")\n",
    "        # print(\"-----------------------------------------------------------------\\n\")\n",
    "\n",
    "        avg_baseline += item[\"judgment\"][\"score\"]\n",
    "        avg_accuracy += item[\"judgment\"][\"is_correct\"]\n",
    "\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(f\"BASELINE avg score: {avg_baseline / 231}\")\n",
    "    print(f\"BASELINE avg accuracy: {avg_accuracy / 231}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b073006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "with open(\"scored_random/qwen-3b-raw.json\", \"r\") as baseline:\n",
    "    print(len(json.load(baseline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "63167eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "BASELINE avg score: 4.034632034632034\n",
      "BASELINE avg accuracy: 0.6883116883116883\n"
     ]
    }
   ],
   "source": [
    "with open(f\"scored_short_random/qwen-3b-raw_small.json\", \"r\") as baseline:\n",
    "    baseline = json.load(baseline)\n",
    "\n",
    "    avg_baseline = 0\n",
    "    avg_accuracy = 0\n",
    "\n",
    "    for i, item in enumerate(baseline):\n",
    "        # print(\"-----------------------------------------------------------------\\n\")\n",
    "        # print(f\"Question: {raw[i]['table_data']['Вопрос']}\\n\")\n",
    "        # print(f\"Relevant context: {raw[i]['table_data']['Отрывок из документа']}\\n\")\n",
    "        # print(f\"Reference answer: {raw[i]['table_data']['Ответ']}\\n\")\n",
    "        # print(f\"RAW answer: {raw[i]['model_answer']}\\n\")\n",
    "        # print(f\"RAW judgment: {raw[i]['judgment']}\\n\")\n",
    "        # print(f\"BASELINE answer: {baseline[i]['model_answer']}\\n\")\n",
    "        # print(f\"BASELINE judgment: {baseline[i]['judgment']}\\n\")\n",
    "        # print(\"-----------------------------------------------------------------\\n\")\n",
    "\n",
    "        avg_baseline += item[\"judgment\"][\"score\"]\n",
    "        avg_accuracy += item[\"judgment\"][\"is_correct\"]\n",
    "\n",
    "    print(\"\\n\\n\\n\")\n",
    "    print(f\"BASELINE avg score: {avg_baseline / 231}\")\n",
    "    print(f\"BASELINE avg accuracy: {avg_accuracy / 231}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
