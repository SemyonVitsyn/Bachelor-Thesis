{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613d781b",
   "metadata": {},
   "source": [
    "# QWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9fe15bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 22:53:25.848788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750017205.868462   75526 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750017205.874447   75526 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750017205.889926   75526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750017205.889939   75526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750017205.889941   75526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750017205.889943   75526 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-15 22:53:25.894780: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d52ae1b228c4f079cb285c2d0e4b29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Шаг 1: Загрузите исходную базовую модель (должна быть доступна!)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"qwen1_5b-v100-webglm-final/merged_model\",  # Исходная модель\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Шаг 2: Добавьте LoRA-адаптеры\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    \"qwen1_5b-v100-webglm-raft/checkpoint-225\",  # Путь к папке с адаптерами\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11269fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3ca7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qwen1_5b-v100-webglm-raft/merged_model/tokenizer_config.json',\n",
       " 'qwen1_5b-v100-webglm-raft/merged_model/special_tokens_map.json',\n",
       " 'qwen1_5b-v100-webglm-raft/merged_model/vocab.json',\n",
       " 'qwen1_5b-v100-webglm-raft/merged_model/merges.txt',\n",
       " 'qwen1_5b-v100-webglm-raft/merged_model/added_tokens.json',\n",
       " 'qwen1_5b-v100-webglm-raft/merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model = model.merge_and_unload()  # <-- Ключевой метод!\n",
    "merged_model.save_pretrained(\"qwen1_5b-v100-webglm-raft/merged_model\")\n",
    "tokenizer.save_pretrained(\"qwen1_5b-v100-webglm-raft/merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3fecf",
   "metadata": {},
   "source": [
    "# LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2ea254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 21:06:41.928441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746814001.947554   98886 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746814001.953416   98886 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746814001.969778   98886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746814001.969793   98886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746814001.969795   98886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746814001.969797   98886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-09 21:06:41.974933: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Шаг 1: Загрузите исходную базовую модель (должна быть доступна!)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"llama-v100-bs_12_2/merged_model\",  # Исходная модель\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Шаг 2: Добавьте LoRA-адаптеры\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    \"llama-v100-bs_12_2-webglm_ft/pretrain_save\",  # Путь к папке с адаптерами\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8e046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f10ce20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama-v100-bs_12_2-webglm_ft/merged_model/tokenizer_config.json',\n",
       " 'llama-v100-bs_12_2-webglm_ft/merged_model/special_tokens_map.json',\n",
       " 'llama-v100-bs_12_2-webglm_ft/merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model = model.merge_and_unload()  # <-- Ключевой метод!\n",
    "merged_model.save_pretrained(\"llama-v100-bs_12_2-webglm_ft/merged_model\")\n",
    "tokenizer.save_pretrained(\"llama-v100-bs_12_2-webglm_ft/merged_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
