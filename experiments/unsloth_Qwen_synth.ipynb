{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e92bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 17:24:51.341955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746973491.366554   18910 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746973491.374189   18910 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746973491.394744   18910 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746973491.394764   18910 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746973491.394767   18910 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746973491.394770   18910 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-11 17:24:51.401993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d674eb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Qwen2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267e3397759d45d287ac814e2128d174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 7150\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=False,\n",
    "    full_finetuning=False,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b96f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora=False,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1420cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def read_jsonl(file_name):\n",
    "    with open(file_name, encoding=\"utf-8\") as r:\n",
    "        return [json.loads(line) for line in r]\n",
    "        # return json.load(r)\n",
    "    \n",
    "simple_data = read_jsonl(\"../Semyon/data/train/synth_needle_simple_train.jsonl\")\n",
    "simple_val_data = read_jsonl(\"../Semyon/data/val/synth_needle_simple_val.jsonl\")\n",
    "simple_dataset = Dataset.from_list(simple_data)\n",
    "simple_val_dataset = Dataset.from_list(simple_val_data)\n",
    "\n",
    "\n",
    "hard_data = read_jsonl(\"../Semyon/data/train/synth_needle_hard_train.jsonl\")\n",
    "hard_val_data = read_jsonl(\"../Semyon/data/val/synth_needle_hard_val.jsonl\")\n",
    "hard_dataset = Dataset.from_list(hard_data)\n",
    "hard_val_dataset = Dataset.from_list(hard_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cafea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724d657def6745b59d2b2c610051a36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa881d4d588431dae945e2b91f98bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# custom_system_message = {\n",
    "#     \"role\": \"system\", \n",
    "#     \"content\": \"Ты — экспертная система Compressa RAG. Предоставляющая точные и релевантные ответы на вопросы.\"\n",
    "# }\n",
    "\n",
    "# # def generate_conversation(examples):\n",
    "# #     conversations = []\n",
    "# #     for messages in examples[\"messages\"]:\n",
    "# #         formatted_messages = [\n",
    "# #             {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
    "# #             for msg in messages\n",
    "# #         ]\n",
    "# #         conversations.append(formatted_messages)\n",
    "# #     return {\"conversations\": conversations}\n",
    "\n",
    "# def generate_conversation(examples):\n",
    "#     conversations = []\n",
    "#     for messages in examples[\"messages\"]:\n",
    "#         formatted_messages = [custom_system_message] + [  # <- кастомный промпт\n",
    "#             {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
    "#             for msg in messages\n",
    "#         ]\n",
    "#         conversations.append(formatted_messages)\n",
    "#     return {\"conversations\": conversations}\n",
    "\n",
    "# train = tokenizer.apply_chat_template(\n",
    "#     dataset.map(generate_conversation, batched=True)[\"conversations\"],\n",
    "#     tokenize=False,\n",
    "# )\n",
    "\n",
    "# val = tokenizer.apply_chat_template(\n",
    "#     val_dataset.map(generate_conversation, batched=True)[\"conversations\"],\n",
    "#     tokenize=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d85166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_train = tokenizer.apply_chat_template(\n",
    "    simple_dataset[\"messages\"],\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "simple_val = tokenizer.apply_chat_template(\n",
    "    simple_val_dataset[\"messages\"],\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "hard_train = tokenizer.apply_chat_template(\n",
    "    hard_dataset[\"messages\"],\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "hard_val = tokenizer.apply_chat_template(\n",
    "    hard_val_dataset[\"messages\"],\n",
    "    tokenize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d60628ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "simple_train_tmp = pd.Series(simple_train)\n",
    "simple_val_tmp = pd.Series(simple_val)\n",
    "\n",
    "simple_train_tmp.name = \"text\"\n",
    "simple_val_tmp.name = \"text\"\n",
    "\n",
    "simple_train_dataset = Dataset.from_pandas(pd.DataFrame(simple_train_tmp))\n",
    "simple_train_dataset = simple_train_dataset.shuffle(seed = 3407)\n",
    "simple_val_dataset = Dataset.from_pandas(pd.DataFrame(simple_val_tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hard_train_tmp = pd.Series(hard_train)\n",
    "hard_val_tmp = pd.Series(hard_val)\n",
    "\n",
    "hard_train_tmp.name = \"text\"\n",
    "hard_val_tmp.name = \"text\"\n",
    "\n",
    "hard_train_dataset = Dataset.from_pandas(pd.DataFrame(hard_train_tmp))\n",
    "hard_train_dataset = hard_train_dataset.shuffle(seed = 3407)\n",
    "hard_val_dataset = Dataset.from_pandas(pd.DataFrame(hard_val_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd55920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_avg 6727.907\n",
      "tokens_max 7102\n",
      "tokens_q95 6937\n",
      "tokens_q99 7013\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tokens_count = 0\n",
    "arr = []\n",
    "\n",
    "for idx, line in enumerate(simple_train):\n",
    "    l = len(tokenizer.encode(line, add_special_tokens=False))\n",
    "    tokens_count += l\n",
    "    arr.append(l)\n",
    "\n",
    "arr = np.sort(np.array(arr))\n",
    "\n",
    "print(f\"tokens_avg {tokens_count / len(simple_train)}\")\n",
    "print(f\"tokens_max {arr[-1]}\")\n",
    "print(f\"tokens_q95 {arr[int(len(simple_train)*0.95)]}\")\n",
    "print(f\"tokens_q99 {arr[int(len(simple_train)*0.99)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e67ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_avg 6518.032\n",
      "tokens_max 7087\n",
      "tokens_q95 6814\n",
      "tokens_q99 6939\n"
     ]
    }
   ],
   "source": [
    "tokens_count = 0\n",
    "arr = []\n",
    "\n",
    "for idx, line in enumerate(hard_train):\n",
    "    l = len(tokenizer.encode(line, add_special_tokens=False))\n",
    "    tokens_count += l\n",
    "    arr.append(l)\n",
    "\n",
    "arr = np.sort(np.array(arr))\n",
    "\n",
    "print(f\"tokens_avg {tokens_count / len(hard_train)}\")\n",
    "print(f\"tokens_max {arr[-1]}\")\n",
    "print(f\"tokens_q95 {arr[int(len(hard_train)*0.95)]}\")\n",
    "print(f\"tokens_q99 {arr[int(len(hard_train)*0.99)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd35526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(hard_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "362c7ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13eb8b70212e407aba0c1eeab756f394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=20):   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b857325958654c8f87bcd60f934c71b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=20):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = hard_train_dataset,\n",
    "    eval_dataset = hard_val_dataset,\n",
    "    packing = False,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 3,\n",
    "        gradient_accumulation_steps = 6,\n",
    "        per_device_eval_batch_size = 2,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 5e-5,\n",
    "        logging_steps = 2,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to=\"wandb\",\n",
    "        output_dir=\"qwen1_5b-v100-synth\",\n",
    "        eval_steps=20,\n",
    "        eval_strategy=\"steps\",\n",
    "        dataloader_num_workers=8\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b5792d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,500 | Num Epochs = 1 | Total steps = 83\n",
      "O^O/ \\_/ \\    Batch size per device = 3 | Gradient accumulation steps = 6\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (3 x 6 x 1) = 18\n",
      " \"-____-\"     Trainable parameters = 18,464,768/1,562,179,072 (1.18% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mseba-vicin\u001b[0m (\u001b[33mseba-vicin-oxford\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Alex/wandb/run-20250511_172537-49h7f3z2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seba-vicin-oxford/huggingface/runs/49h7f3z2' target=\"_blank\">qwen1_5b-v100-synth</a></strong> to <a href='https://wandb.ai/seba-vicin-oxford/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seba-vicin-oxford/huggingface' target=\"_blank\">https://wandb.ai/seba-vicin-oxford/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seba-vicin-oxford/huggingface/runs/49h7f3z2' target=\"_blank\">https://wandb.ai/seba-vicin-oxford/huggingface/runs/49h7f3z2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='83' max='83' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [83/83 56:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.294100</td>\n",
       "      <td>1.291836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.271600</td>\n",
       "      <td>1.269974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.264000</td>\n",
       "      <td>1.263020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.262600</td>\n",
       "      <td>1.262058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d7ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"qwen1_5b-v100-synth/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "110e310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"qwen1_5b-v100-synth/pretrain_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a846a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
