{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38c990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import faiss\n",
    "import torch\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7f1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    _shared_model = None\n",
    "    _shared_tokenizer = None\n",
    "\n",
    "    def __init__(self, folder_path: str, benchmark_path: str, retriever_model: str = \"intfloat/multilingual-e5-large\", batch_size: int = 12):\n",
    "        self.folder_path = folder_path\n",
    "        self.benchmark_path = benchmark_path\n",
    "        self.retriever_model = retriever_model\n",
    "        self.batch_size = batch_size \n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if DocumentRetriever._shared_model is None:\n",
    "            DocumentRetriever._shared_tokenizer = AutoTokenizer.from_pretrained(retriever_model)\n",
    "            DocumentRetriever._shared_model = AutoModel.from_pretrained(retriever_model).to(self.device)\n",
    "            DocumentRetriever._shared_model.eval()\n",
    "\n",
    "        self.model = DocumentRetriever._shared_model\n",
    "        self.tokenizer = DocumentRetriever._shared_tokenizer\n",
    "\n",
    "        self.chunks: List[Tuple[str, str]] = []\n",
    "        self.index = None\n",
    "        self._text_splitter = self._create_text_splitter()\n",
    "\n",
    "    def __del__(self):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def _create_text_splitter(self):\n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=300,\n",
    "            chunk_overlap=50,\n",
    "            length_function=self._token_counter,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\"]\n",
    "        )\n",
    "\n",
    "    def _token_counter(self, text: str) -> int:\n",
    "        return len(self.tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    def _read_md_files(self) -> List[Tuple[str, str]]:\n",
    "        files = glob.glob(os.path.join(self.folder_path, \"*.md\"))\n",
    "        documents = []\n",
    "        for file_path in files:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "                documents.append((doc_name, f.read()))\n",
    "        return documents\n",
    "\n",
    "    def _split_documents(self, documents: List[Tuple[str, str]]):\n",
    "        self.chunks = []\n",
    "        for doc_name, content in documents:\n",
    "            for chunk in self._text_splitter.split_text(content):\n",
    "                self.chunks.append((doc_name, chunk))\n",
    "\n",
    "    @staticmethod\n",
    "    def _average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "    def _create_embeddings(self, texts: List[Tuple[str, str]], is_queries: bool = False) -> np.ndarray:\n",
    "        embeddings = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(texts), self.batch_size)):\n",
    "                batch = texts[i:i+self.batch_size]\n",
    "\n",
    "                if is_queries:\n",
    "                    prefixed_batch = [\"query: \" + text for text in batch]\n",
    "                else:\n",
    "                    prefixed_batch = [f\"passage: {text}\" for doc_name, text in batch]\n",
    "\n",
    "                inputs = self.tokenizer(\n",
    "                    prefixed_batch,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=512\n",
    "                ).to(self.device)\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                batch_embeddings = self._average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "                batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "                embeddings.append(batch_embeddings.cpu().numpy())\n",
    "\n",
    "                del inputs, outputs\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "    def build_index(self):\n",
    "        documents = self._read_md_files()\n",
    "        self._split_documents(documents)\n",
    "        embeddings = self._create_embeddings(self.chunks)\n",
    "\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)\n",
    "        self.index.add(embeddings.astype(np.float32))\n",
    "\n",
    "    def search_df(self, df: pd.DataFrame, top_k: int) -> List[Dict[str, Any]]:\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Индекс не построен. Сначала вызовите build_index()\")\n",
    "\n",
    "        query_embeddings = self._create_embeddings(df['Вопрос'].tolist(), is_queries=True)\n",
    "        scores, indices = self.index.search(query_embeddings.astype(np.float32), top_k)\n",
    "\n",
    "        results = []\n",
    "        for i in range(len(df)):\n",
    "            row = df.iloc[i]\n",
    "            query_scores = scores[i]\n",
    "            query_indices = indices[i]\n",
    "            \n",
    "            result = {\n",
    "                \"table_data\": {\n",
    "                    \"Домен документов\": row[\"Домен документов\"],\n",
    "                    \"Сет документов\": row[\"Сет документов\"],\n",
    "                    \"Название документа\": row[\"Название документа\"],\n",
    "                    \"Отрывок из документа\": row[\"Отрывок из документа\"],\n",
    "                    \"Тип вопроса\": row[\"Тип вопроса\"],\n",
    "                    \"Вопрос\": row[\"Вопрос\"],\n",
    "                    \"Ответ\": row[\"Ответ\"]\n",
    "                },\n",
    "                \"context\": []\n",
    "            }\n",
    "\n",
    "            for idx, score in zip(query_indices, query_scores):\n",
    "                doc_name, chunk_text = self.chunks[idx]\n",
    "                result[\"context\"].append({\n",
    "                    \"score\": float(score),\n",
    "                    \"doc\": doc_name,\n",
    "                    \"text\": chunk_text\n",
    "                })\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search(self, queries: List[str], top_k: int) -> List[Dict[str, Any]]:\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Индекс не построен. Сначала вызовите build_index()\")\n",
    "\n",
    "        query_embeddings = self._create_embeddings(queries, is_queries=True)\n",
    "        scores, indices = self.index.search(query_embeddings.astype(np.float32), top_k)\n",
    "\n",
    "        results = []\n",
    "        for i, query in enumerate(queries):\n",
    "            query_scores = scores[i]\n",
    "            query_indices = indices[i]\n",
    "            \n",
    "            result = {\n",
    "                \"question\": query,\n",
    "                \"context\": []\n",
    "            }\n",
    "\n",
    "            for idx, score in zip(query_indices, query_scores):\n",
    "                doc_name, chunk_text = self.chunks[idx]\n",
    "                result[\"context\"].append({\n",
    "                    \"score\": float(score),\n",
    "                    \"doc\": doc_name,\n",
    "                    \"text\": chunk_text\n",
    "                })\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def read_benchmark(self) -> pd.DataFrame:\n",
    "        set_name = os.path.basename(self.folder_path.rstrip(\"/\\\\\"))\n",
    "        df = pd.read_csv(self.benchmark_path)\n",
    "        return df[df[\"Сет документов\"] == set_name].reset_index(drop=True)\n",
    "\n",
    "    def search_for_benchmark(self, top_k=5) -> Tuple[pd.DataFrame, List[Dict[str, Any]]]:\n",
    "        self.build_index()\n",
    "\n",
    "        df = self.read_benchmark()\n",
    "        results = self.search_df(df, top_k)\n",
    "        return df, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add06d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Нефтегаз...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:17<00:00,  3.84it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Медицина...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:20<00:00,  3.84it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Металлургия...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [00:35<00:00,  3.82it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 13.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# small, medium, large\n",
    "# 5, 10, 20\n",
    "\n",
    "base_folder = \"benchmark_short\"\n",
    "benchmark_path = \"benchmark_short/benchmark_short.csv\"\n",
    "all_results = []\n",
    "\n",
    "retrievers = {}\n",
    "for folder_name in os.listdir(base_folder):\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Processing {folder_name}...\")\n",
    "        retriever = DocumentRetriever(\n",
    "            folder_path=folder_path,\n",
    "            benchmark_path=benchmark_path,\n",
    "            batch_size=16\n",
    "        )\n",
    "        \n",
    "        df, results = retriever.search_for_benchmark(top_k=20)\n",
    "        all_results += results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "169c5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generation/benchmark/benchmark_short_large.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0152f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\", padding_side='left')\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG. Предоставляющая точные и релевантные ответы на вопросы.\"}\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\\n\"\n",
    "        \"1. Дай краткий ответ на вопрос, используя только информацию из контекста.\\n\"\n",
    "        f\"2. Если ответа на вопрос нет в источниках, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        f\"# Вопрос:\\n\\n{question}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c069f750",
   "metadata": {},
   "source": [
    "# small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26a038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00, 12.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_avg 2099.926406926407\n",
      "tokens_max 2712\n",
      "tokens_q95 2443\n",
      "tokens_q99 2669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"generation/benchmark/benchmark_short_small.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "tokens_count = 0\n",
    "arr = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), 16)):\n",
    "    batch = benchmark_data[i:i+16]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    for line in chat_prompts:\n",
    "        l = len(tokenizer.encode(line, add_special_tokens=False))\n",
    "        tokens_count += l\n",
    "        arr.append(l)\n",
    "\n",
    "\n",
    "arr = np.sort(np.array(arr))\n",
    "\n",
    "print(f\"tokens_avg {tokens_count / len(benchmark_data)}\")\n",
    "print(f\"tokens_max {arr[-1]}\")\n",
    "print(f\"tokens_q95 {arr[int(len(benchmark_data)*0.95)]}\")\n",
    "print(f\"tokens_q99 {arr[int(len(benchmark_data)*0.99)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede90a8",
   "metadata": {},
   "source": [
    "# medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6faabb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_avg 4002.4891774891776\n",
      "tokens_max 4800\n",
      "tokens_q95 4581\n",
      "tokens_q99 4775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"generation/benchmark/benchmark_short_medium.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "tokens_count = 0\n",
    "arr = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), 16)):\n",
    "    batch = benchmark_data[i:i+16]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    for line in chat_prompts:\n",
    "        l = len(tokenizer.encode(line, add_special_tokens=False))\n",
    "        tokens_count += l\n",
    "        arr.append(l)\n",
    "\n",
    "\n",
    "arr = np.sort(np.array(arr))\n",
    "\n",
    "print(f\"tokens_avg {tokens_count / len(benchmark_data)}\")\n",
    "print(f\"tokens_max {arr[-1]}\")\n",
    "print(f\"tokens_q95 {arr[int(len(benchmark_data)*0.95)]}\")\n",
    "print(f\"tokens_q99 {arr[int(len(benchmark_data)*0.99)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264e0d7",
   "metadata": {},
   "source": [
    "# large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27d7f77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:05<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_avg 7785.666666666667\n",
      "tokens_max 9010\n",
      "tokens_q95 8681\n",
      "tokens_q99 8969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"generation/benchmark/benchmark_short_large.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "tokens_count = 0\n",
    "arr = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), 16)):\n",
    "    batch = benchmark_data[i:i+16]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    for line in chat_prompts:\n",
    "        l = len(tokenizer.encode(line, add_special_tokens=False))\n",
    "        tokens_count += l\n",
    "        arr.append(l)\n",
    "\n",
    "\n",
    "arr = np.sort(np.array(arr))\n",
    "\n",
    "print(f\"tokens_avg {tokens_count / len(benchmark_data)}\")\n",
    "print(f\"tokens_max {arr[-1]}\")\n",
    "print(f\"tokens_q95 {arr[int(len(benchmark_data)*0.95)]}\")\n",
    "print(f\"tokens_q99 {arr[int(len(benchmark_data)*0.99)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08e13b",
   "metadata": {},
   "source": [
    "# reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "921f29db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:00<00:00, 501233.43it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"generation/benchmark/benchmark_short_large.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data))):\n",
    "    item = benchmark_data[i]\n",
    "    item['context'].reverse()\n",
    "    results.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6dfd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generation_reverse/benchmark/benchmark_short_large.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd143a29",
   "metadata": {},
   "source": [
    "# random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9f512bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:00<00:00, 78771.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "with open(\"generation/benchmark/benchmark_short_large.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data))):\n",
    "    item = benchmark_data[i]\n",
    "    random.shuffle(item['context'])\n",
    "    results.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "840e1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generation_random/benchmark/benchmark_short_large.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
