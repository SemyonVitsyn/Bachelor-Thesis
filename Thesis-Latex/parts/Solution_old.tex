\section{Построение решения задачи}
\label{sec:Solution} \index{Chapter5}

Для оптимизации времени разработки эффективного подхода было принято решения разделить процесс обучения на несколько ключевых этапов и в дальнейшем оценить влияние каждого из них.

\begin{enumerate}

  \item \textbf{\textit{Адаптация к русскому языку.}}

    Современные языковые модели обучаются на обширных мультиязычных текстовых корпусах, что обеспечивает их базовую функциональность во многих сценариях применения. Однако доминирование англоязычных данных в обучающих выборках приводит в выраженному ухудшению качество генерации и понимания для языков с меньшей репрезентацией, включая русский.

    Для решения этой задачи в исследовании было реализовано дообучение модели на русскоязычном датасете Saiga, а оценка эффективности была проведена на тестовом наборе Massive Multitask Language Understanding (MMLU) \cite{MMLU}, адаптированном под русский язык.
  
 \item \textit{\textbf{Улучшение качества контекстно-зависимой генерации.}}
  
    Следующим этапом стал сравнительный анализ различных подходов адаптации языковой модели к задаче контекстной генерации. В качестве обучающего набора выступил корпус WebGLM-QA, разработанный для web-enhanced question-answering system based on the General Language Model (WebGLM) \cite{WebGLM}. Предварительно данные были переведены на русский язык. Главной целью было провести верификацию эффективности подхода RAFT. Для численной оценки результатов был собран специальный бенчмарк, на котором проводилась сравнение качества генерации с помощью ROUGE и нейросетевой модели-судьи.  
  
  \item \textit{\textbf{Адаптация к большому контексту.}}
  
    Для решения проблемы <<lost in the middle>> \cite{lost_in_the_middle}, а также увеличения размера эффективного контекста модели, было проведено дообучение на синтетических данных с размером контекста порядка 7-8 тысяч токенов. Оценка эффективности подхода была проведена на части тестовой выборки датасета WebGLM с искуственным увеличением размера контекста.
  
\end{enumerate}

\subsection{Базовые модели}
\label{subsec:base_models} \index{Chapter5!Base_models}

В силу ориентированности работы на задачу RAG с большим количеством входных токенов и ограниченности вычислительных ресурсов, выбор моделей был ограничен архитектурами в диапазоне 1–3 млрд параметров.

Основными исследуемыми архитектурой стали модели Qwen2.5-1.5B-Instruct и Qwen2.5-3B-Instruct от Qwen team, Alibaba Cloud. Несмотря на официальное отсутствие поддержки русского языка, на практике модели эффективно справляется с мультиязычными задачами. А также согласно различным независимым рейтингам (например, Open LLM Leaderboard и MERA Leaderboard), являются одними из самых эффективных в своем весовой категории для множества стандартных задач.

% \textcolor{red}{\large\textit{УБРАТЬ, ЕСЛИ НЕ ХВАТИТ ВРЕМЕНИ}}

% Для кросс-архитектурной валидации методологии дообучения дополнительно использовалась модель Llama-3.2-1B-Instruct от компании Meta AI. Выбор был обусловлен её широким распространением, меньшим размером и низкой изначальной адаптацией к русскому языку.

\subsection{Saiga}
\label{subsec:saiga} \index{Chapter5!Saiga}

Главной проблемой при руссификации моделей остается недостаточное количество качественных русскоязычных данных для обучения. Датасет \\ \verb|Saiga_scored| представляет собой смесь русскоязычных Question answering (QA) запросов из различных источников. Он состоит из 42 тысяч тренировочных примеров, взятых как их мультиязычных датасетов, содержащих русский, так и синтетически сгенерированных ответов других языковых моделей.  

\subsection{WebGLM-QA}
\label{subsec:webglm} \index{Chapter5!webglm}

Текстовый корпус WebGLM-QA содержит 44 тысячи QA пар с релевантными источниками. В ответе также содержатся указания на используемые источники, что по задумке должно научить модель эффективнее использовать контекст. Перевод осуществлялся с помощью gpt.

\newpage

\subsection{Синтетические данные}
\label{subsec:synth_data} \index{Chapter5!synth_data}

В качестве данных для финального этапа дообучения были созданы синтетические обучающие наборы, согласно подходу из статьи <<From Artificial Needles to Real Haystacks: Improving
Retrieval Capabilities in LLMs by Finetuning on
Synthetic Data>> \cite{synth_needle}. Этот процесс делится на 2 этапа:

\begin{enumerate}
  
  \item \textit{\textbf{Simple.}}

    Генерируем список словарей, в которых каждый ключ и значение являются целыми числами. Выбирая произвольный уникальный ключ, ставим модели задачу найти значение по ключу, а также номер словаря, в котором он находится. Ответ в формате json с полями <<значение>> и <<номер\textunderscoreсловаря>>. Размер сгенерированной обучающей выборки составил 1000 примером.
  
  \item \textit{\textbf{Multi-subkey.}}

    Задача аналогична предыдущей за исключением того, что каждый ключ представляет собой кортеж целых чисел, а порядок элементов внутри перемешан. Размер сгенерированной обучающей выборки составил 1500 примером.
  
\end{enumerate}

Такая прогрессия существует по причине того, что для модели с 1 миллиардом параметров на большом контексте эта задача может оказаться слишком сложной.

Более детально процесса формирования обучающих данных будет описан в главе 7.

\newpage




% \section{Методы оценки качества}
% \label{sec:Scoring} \index{Chapter 6}

% Для оценки качества RAG-системы нужна специализированная процедура. На текущий момент в открытом доступе отсутствуют русскоязычные бенчмарки, в полной мере отвечающие всем требованиям. Во многом это обусловлено отсутствием стандартизированных метрик качества контекстно-зависимой QA генерации. Классические статистические методы не подходят для всесторонней оценки реального качества, а крупные коммерческие компании чаще всего используют дорогостоящую человеческую разметку. В связи с этим актуальной становится задача создания бенчмарка,  максимально приближенного к практическим сценариям применения RAG.

% Далее в этой главе будут описаны все использованные подходы: от базовой верификации качества генерации на русском языке до построения собственного бенчмарка и использования его для оценки выстроенной RAG-системы.

% \textcolor{red}{ДОБАВИТЬ, ЧТО МЕТРИКА ЭТО ACCURACY, Т.Е. ПРОСТО ДОЛЯ ВЕРНЫХ ОТВЕТОВ, А ТАКЖЕ ЧТО В САМОМ БЕНЧМАРКЕ ВСЕГО 4 ВАРИАНТА ОТВЕТА НА КАЖДЫЙ ВОПРОС.}

% \subsection{MMLU}
% \label{subsec:mmlu} \index{Chapter6!mmlu}

% Среди существующих инструментов оценки языковых моделей для русского языка выделяется мультимодальный бенчмарк MERA (Multimodal Evaluation of Russian-language Architectures) \cite{MERA}. Этот набор является собранием различных задач, включая адаптированную для русского языка версию MMLU.

% Оригинальный MMLU предназначен для измерения профессиональных знаний модели, приобретенных в процессе обучения в различных областях. Задачи охватывают 57 областей знаний по различным темам (доменам): гуманитарные науки, математика, инженерия и другие.

% Из-за большого разнообразия затрагиваемых тем в данной работе набор использовался для оценки эффективности адаптации архитектур к русскому языку. 

% \newpage

% \begin{Verbatim}[frame=single, fontsize=\small]
% **Question**: {question}
% **Relevant context**: {relevant_context}
% **Reference Answer**: {reference_answer}
% **Model Answer**: {model_answer}

% **Evaluation Rules**:
% 1. Scoring Scale:
% - 5: Good answer (factually correct, complete)
% - 4: Mostly correct with minor issues
% - 3: Partially correct but has inaccuracies
% - 2: Mostly incorrect but contains relevant elements
% - 1: Completely wrong or irrelevant

% 2. Binary Correctness:
% - 1 (Correct): Answer conveys the same meaning as reference 
% (exact wording not required)
% - 0 (Incorrect): Meaning differs from reference or contains false information

% 3. Binary Refuse:
% - 1 (Inappropriate refusal): The model answer is a refusal, \
% while the reference contains a full answe
% - 0 (A normal answer or an appropriate refusal)

% 4. Key Principles:
% - Focus on whether the model answers the question asked, 
% not on whether it exactly matches the reference
% - There is no need to lower the score if the model's answer
% omits some facts that are insignificant to the question
% - If an answer seems redundant in relation to the reference,
% but the information is contained in a relevant context,
% then there is no need to lower the score for this
% - Ignore stylistic differences if core meaning is preserved
% - Inappropriate refusal should always have score 2 out of 5!

% Provide analysis in JSON format: {{"comment": "...", "score": 1-5, 
% "is_correct": 0|1, "is_inappropriate_refusal": 0|1}}
% \end{Verbatim}
