\section{Описание практической части}
\label{Practical} \index{Chapter5}

В силу ориентированности работы на задачу RAG с большим количеством входных токенов и ограниченности вычислительных ресурсов, выбор моделей был ограничен архитектурами в диапазоне 1–3 млрд параметров.

Основными исследуемыми архитектурой стали модели Qwen2.5-1.5B-Instruct и Qwen2.5-3B-Instruct от Qwen team, Alibaba Cloud. Несмотря на официальное отсутствие поддержки русского языка, на практике модели эффективно справляется с мультиязычными задачами. А также согласно различным независимым рейтингам (например, Open LLM Leaderboard и MERA Leaderboard), являются одними из самых эффективных в своем весовой категории для множества стандартных задач.

% В частности, будем использовать открытые предобученные инструкционные модели. После этапа языкового моделирования на большом объеме данных они дополнительно обучаются на задачах, требующих следовать определенным инструкциям. Это необходимо для адаптации модели к формату вопрос-ответ, а также улучшению качества генерации при специализированном запросе.


Процесс обучения больших языковых моделей требует большого количества вычислительных ресурсов и времени. Для проведения всей экспериментальной части в этом исследовании использовалась следующая конфигурация:
\begin{itemize}
    \item GPU NVIDIA Tesla V100 PCIe 32 GB.

    \item 20 VCPU.

    \item 64 GB RAM.

    \item 400 GB SSD.
\end{itemize}



\subsection{Построение RAG пайплайна} 
\label{subsec:rag_pipeline} \index{Chapter5!RAG_pipeline}


\begin{enumerate}
  
  \item \textit{\textbf{Обработка документов}}

    Исходно все документы бенчмарка были представлены в формате pdf, однако он не подходит для чанкования и подачи в контекст LLM из-за своей структуры. По этой причине все документы были переведены в формат markdown, что позволило сохранить общую структуру документы (подразделы, табличные данных) и преобразовать данные в исключительно текстовый формат. Для этого этапа была использована библиотека docling.
  
  \item \textit{\textbf{Подготовка инфраструктуры}}

    В качестве embedder модели использовался intfloat/multilingual-e5-large. Данная модель показывает наилучшее качество на бенчмарке ruMTEB и полностью подходит для нашей задачи.

    При чанковании текст разделялся на фрагменты по 300 токенов, что составляет примерно 10-20 предложений, в зависимости от их длины. Разделение проводилось по знакам препинания с перекрытием 50 токенов между соседними фрагментами текста. 

    Так как в каждом домене бенчмарка было 3-10 документов, то не было необходимости использовать полноценную векторную базу данных. Для построения тематических индексов использовалась библиотека FAISS.

  \item \textit{\textbf{Ретрив}}

    При генерации контекста к запросам бенчмарка, использовались top-10 наиболее близких документа. Итоговый контекст запроса к модели составлял порядка 4-5 тысяч токенов. Для корректного сравнения различных генеративных моделей набор документов фиксировался. Качество ретрива высокое, в 85\% случаев релевантный фрагмент находился в top-5 документах.

\end{enumerate}



\subsection{Процесс формирования бенчмарка}
\label{subsec:benchmark_practice} \index{Chapter5!benchmark_practice}

Создание бенчмарка можно разделить на следующие этапы:

\begin{enumerate}
    \item \textbf{\textit{Формирование индексов и поиск документов}}

        Данный этап является ключевым. Он должен быть тщательно спланировал и реализован полностью вручную, отражая реальный потребности технологии.

    \item \textbf{\textit{Типизация вопросов}}

        В нашей работе использовалась типизация на 10 классов, где вопросы <<Multi block>> и <<Logical thinking>> считались усложненными, так как требовали модели дополнительного понимания контекста, а не только извлечения релевантного фрагмента текста. Наша классификация покрывает большинство сценариев применения RAG-систем.

    \item \textbf{\textit{Создание примеров}}

        Сперва необходимо вручную найти независимый фрагмент документа, к которому можно задать правдоподобный вопрос. Это делалось вручную и таким образом формировались вопросы типа <<Simple>>. Соответствующие этому вопросу типы <<With errors>>, <<Trash>>, <<Reformulation>>, <<Incorrect by design>> формировались автоматически через запросы к языковой модели, в частности использовались DeepSeek-R1 и GPT-4o.

        Вопросы <<No Info>> генерировались автоматически, путем отправки нескольких глав документа языковой модели. Оставшиеся типы вопросов генерировались полностью вручную.

    \item \textbf{\textit{Верификация}}

        При формировании итогового бенчмарка важно просмотреть все примеры и очистить данные от мусора.
    
\end{enumerate}

Данный подход позволяет значительно оптимизировать время разработки без существенных потерь в итоговом качестве данных.



\subsection{Конфигурации обучения}
\label{subsec:config} \index{Chapter5!config}

При реализации процесса дообучения использовались следующие гиперпараметры:  

\begin{table}[ht]
\centering
\caption{Гиперпараметры}
\fontsize{12}{14}\selectfont
\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{
  >{\centering\arraybackslash}p{5cm} 
  >{\centering\arraybackslash}p{10cm}
}
\toprule
\textbf{Параметр} & \textbf{Диапазон} \\
\midrule
LoRA rank & 16 \\
\midrule
LoRA alpha & 8 - 16 \\
\midrule
Learning rate & 0.00001 - 0.00004 \\
\midrule
Weight decay & 0.01 \\
\midrule
Global batch size & 24 - 32 \\
\midrule
Warmup fraction & 10\% \\
\midrule
Learning rate scheduler & linear \\
\midrule
Optimizer & AdamW 8bit \\
\bottomrule
\end{tabularx}
\end{table}


Через LoRA адаптеры обучались attention и feedforward network блоки, остальные компоненты модели не модифицировались: 

\begin{lstlisting}
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    lora_alpha=16,
    lora_dropout=0,
    target_modules=["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"],
)
\end{lstlisting}

Обучение проводилось с использованием библиотеки unsloth. Конфигурации для каждого из типов обучения находятся в приложении C \ref{app:learning_config}.


\subsection{Адаптация к русскому языку}
\label{subsec:ru} \index{Chapter5!ru}

Главной проблемой при руссификации моделей остается недостаточное количество качественных русскоязычных данных для обучения. Датасет \\ \verb|Saiga_scored| представляет собой смесь русскоязычных QA запросов из различных источников.

\begin{table}[ht]
\centering
\caption{Описание Saiga}
\fontsize{12}{14}\selectfont
\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{
  >{\centering\arraybackslash}p{5cm} 
  >{\centering\arraybackslash}p{10cm}
}
\toprule
\textbf{Параметр} & \textbf{Описание} \\
\midrule
Общее количество данных & 42к \\
\midrule
Размер обучающей выборки & 35к \\
\midrule
Размер валидационной выборки & 5к \\
\midrule
Размер тестовой выборки & 2к \\
\midrule
Размер контекста & 2048 токенов \\
\midrule
Содержание & Инструкционные запросы, адаптированные под русский язык (75\% данных на русском языке) \\
\midrule
Источник данных & Смешанный - есть как примеры из мультиязычных датасетов, содержащих русский, так и синтетически сгенерированные ответы других языковых моделей \\
\bottomrule
\end{tabularx}
\end{table}

Среди существующих инструментов оценки языковых моделей для русского языка выделяется мультимодальный бенчмарк MERA (Multimodal Evaluation of Russian-language Architectures) \cite{MERA}. Этот набор является собранием различных задач, включая адаптированную для русского языка версию MMLU.

Оригинальный MMLU \cite{MMLU} предназначен для измерения профессиональных знаний модели, приобретенных в процессе обучения в различных областях. Задачи охватывают 57 различных тем: гуманитарные науки, математика, инженерия и другие. Данные состоят из вопроса и четырёх вариантов ответа, среди которых только один правильный. Метрика оценки - accuracy.

Из-за большого разнообразия затрагиваемых тем в данной работе набор использовался для оценки эффективности адаптации архитектур к русскому языку. 




\subsection{WebGLM-QA}
\label{subsec:default_webglm} \index{Chapter5!default_webglm}

В качестве обучающего набора для адаптации LLM к задаче контекстной генерации выступил корпус WebGLM-QA, разработанный для web-enhanced question-answering system based on the General Language Model (WebGLM) \cite{WebGLM}. Предварительно данные были адаптированны под русский язык с помощью машинного перевода.

\begin{table}[ht]
\centering
\caption{Описание WebGLM-QA}
\fontsize{12}{14}\selectfont
\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{
  >{\centering\arraybackslash}p{5cm} 
  >{\centering\arraybackslash}p{10cm}
}
\toprule
\textbf{Параметр} & \textbf{Описание} \\
\midrule
Общее количество данных & 45к \\
\midrule
Размер обучающей выборки & 44к \\
\midrule
Размер валидационной выборки & 1000 \\
\midrule
Размер тестовой выборки & 400 \\
\midrule
Размер контекста & 2048 токенов \\
\midrule
Содержание & Запросы с результатами веб-поиска. Имитируют реальные сценарии применения. \\
\midrule
Источник данных & Вопросы взяты из открытых источников, контекст собирался с помощью автоматического веб-поиска, эталонные ответы формировались с помощью языковой модели. В дальнейшем проведена фильтрации данных, путем проверки ссылок и удаления ошибочных фрагментов. \\
\midrule
Особенности & В ответе также содержатся указания на используемые источники, что по задумке должно научить модель эффективнее использовать контекст. \\
\bottomrule
\end{tabularx}
\end{table}

Оценка эффективности проводилась на итоговом бенчмарке.



\subsection{WebGLM-RAFT}
\label{subsec:raft_webglm} \index{Chapter5!raft_webglm}

Для модификации датасета WebGLM-QA под метод RAFT использовалась следующая конфигурация:

\begin{table}[H]
\centering
\caption{Построение WebGLM-RAFT}
\fontsize{12}{14}\selectfont
\renewcommand{\arraystretch}{1.0}
\begin{tabularx}{\textwidth}{
  >{\centering\arraybackslash}p{2.5cm} 
  >{\centering\arraybackslash}p{2.5cm} 
  >{\centering\arraybackslash}p{10cm}
}
\toprule
\textbf{Параметр} & \textbf{Значение} & \textbf{Описание} \\
\midrule
Количество отвлекающих документов & 3 фрагмента & Для добавления шума в контекст генерации модели, помимо релевантного фрагмента, добавлялось еще 3 фрагмента с предыдущих запросов. В последствии порядок источников перемешивался, а номер источников в ответе актуализировался для текущей индексации. \\
\midrule
COT разметка & Llama-3.3-70B-Instruct & Для генерации последовательности рассуждений использовалась Llama-3.3-70B-Instruct через Nebius API. Так как столь громоздкую модель хостить локально не представляется возможным, использовалось API для генерации ответов (Формат запроса указан в приложении B \ref{app:cot_prompt}).\\
\midrule
Доля негативных примеров & 10\% & Дополнительно в датасет было добавлено 5000 примеров без релевантного для запроса контекста. В качестве нерелевантного брался контекст с 4 предыдущих запросов. В качестве эталонного ответа ставился шаблонный отказ.\\
\bottomrule
\end{tabularx}
\end{table}

Итоговая конфигурация данных:

\begin{table}[H]
\centering
\caption{Описание WebGLM-RAFT}
\fontsize{12}{14}\selectfont
\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{
  >{\centering\arraybackslash}p{5cm} 
  >{\centering\arraybackslash}p{10cm}
}
\toprule
\textbf{Параметр} & \textbf{Описание} \\
\midrule
Общее количество данных & 50к \\
\midrule
Размер обучающей выборки & 45к \\
\midrule
Размер валидационной выборки & 3500 \\
\midrule
Размер тестовой выборки & 1500 \\
\midrule
Размер контекста & 4096 токенов \\
\bottomrule
\end{tabularx}
\end{table}

Оценка эффективности проводилась на итоговом бенчмарке.

\newpage



\subsection{Синтетические данные}
\label{subsec:synth_dict} \index{Chapter5!synth_dict}

Для построения синтетических данных была написана функция на python, создающая случайные словари. Детальнее:

\begin{itemize}
    \item 100 словарей из 2-4 пар ключ-значение.

    \item Ключи это кортежи из 3-4 чисел в диапазоне 100-999.

    \item Значения это целые числа в диапазоне 1000-9999.

    \item В начале создаются golden key и golden value, которые в дальнейшем будут учавствовать в запросе к модели. Они помещаются в случаный словарь.

    \item Оставшиеся элементы в словарях генерируются с учётом корректности и единственности golden пары.

    \item В запрос к модели подается перемешанный golden key. Модель должна ответить в json формате, указав оригинальный golden key, соответствующее ему golden value, а также номер словаря, в котором находится пара.
\end{itemize}

\begin{table}[ht]
\centering
\caption{Описание синтетических данных}
\fontsize{12}{14}\selectfont
\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{
  >{\centering\arraybackslash}p{5cm} 
  >{\centering\arraybackslash}p{10cm}
}
\toprule
\textbf{Параметр} & \textbf{Описание} \\
\midrule
Общее количество данных & 1500 \\
\midrule
Размер обучающей выборки & 1400 \\
\midrule
Размер валидационной выборки & 100 \\
\midrule
Размер контекста & 8192 токена \\
\bottomrule
\end{tabularx}
\end{table}

\newpage
