{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a331bd89",
   "metadata": {},
   "source": [
    "# QWEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "600c8645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 21:28:56.848310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747765736.872240   87473 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747765736.879711   87473 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747765736.899633   87473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747765736.899654   87473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747765736.899657   87473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747765736.899660   87473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-20 21:28:56.906724: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedf8a1dace446598d466a9b09079648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"qwen-1.5b-raft\"\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG. Предоставляющая точные и релевантные ответы на вопросы.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model_500\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# \"../Alex/qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "# \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model\"\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\\n\"\n",
    "        \"1. Дай полный и краткий ответ на вопрос, используя только информацию из контекста.\\n\"\n",
    "        f\"2. Если ответа на вопрос нет в источниках, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        f\"# Вопрос:\\n\\n{question}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541044ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [1:47:02<00:00, 149.36s/it]\n"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        # print(batch[j]['table_data']['Вопрос'])\n",
    "        # print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        # print(batch[j]['table_data']['Ответ'])\n",
    "        # print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748482f",
   "metadata": {},
   "source": [
    "# LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a53e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 13:38:23.458442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747219103.482478   13811 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747219103.488781   13811 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747219103.508068   13811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747219103.508084   13811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747219103.508087   13811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747219103.508089   13811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-14 13:38:23.514692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"llama-1b-raw\"\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG. Предоставляющая точные и релевантные ответы на вопросы.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "# \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "# \"../Alex/llama-v100-bs_12_2/merged_model\",\n",
    "# \"../Alex/llama-v100-bs_12_2-webglm_ft/merged_model\",\n",
    "\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\\n\"\n",
    "        \"1. Дай полный и краткий ответ на вопрос, используя только информацию из контекста.\\n\"\n",
    "        f\"2. Если ты уверен, что ответа на вопрос нет в источниках, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        f\"# Вопрос:\\n\\n{question}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4b0761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [01:21<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Какова доля трудноизвлекаемых запасов нефти в России и где расположены основные месторождения?\n",
      "---------------------------------------------------\n",
      "Доля трудноизвлекаемых запасов нефти в России составляет около 55% от общих запасов российской нефти. Основные российские месторождения высоковязкой нефти расположены в Пермской области, Татарстане, Башкирии и Удмуртии. Наиболее крупные из них: Ван Еганское, Северо Комсомольское, Усинское, —Русское, Гремихинское и др. На сегодня добыча высоковязкой нефти, транспортировка ее к пунктам сбора и подготовке и переработка с целью получения конечных продуктов — одна из актуальных задач нефтедобывающей промышленности в связи с ростом доли трудноизвлекаемых запасов нефти в стране.\n",
      "---------------------------------------------------\n",
      "Доля трудноизвлекаемых запасов нефти в России составляет около 55 % от общих запасов российской нефти. Основные месторождения высоковязкой (трудноизвлекаемой) нефти расположены в Пермской области, Татарстане, Башкирии и Удмуртии.\n",
      "\n",
      "Сколька процента нефти в России трудано достать и гдзе ана?\n",
      "---------------------------------------------------\n",
      "К сожалению, ответа на вопрос нет в упомянутых источниках.\n",
      "---------------------------------------------------\n",
      "Доля трудноизвлекаемых запасов нефти в России составляет около 55 % от общих запасов российской нефти. Основные месторождения высоковязкой (трудноизвлекаемой) нефти расположены в Пермской области, Татарстане, Башкирии и Удмуртии.\n",
      "\n",
      "Привет, у меня тут возник вопрос по поводу сложностей нефтедобычи. Знаешь, я слышал, что в России много нефти, но насколько реально её добывать? Наверное большую часть очень сложно добывать, да?!? Особенно интересует, какая часть этой нефти считается трудноизвлекаемой и где её больше всего? Спасибо заранее!\n",
      "---------------------------------------------------\n",
      "К сожалению, ответа на вопрос нет в упомянутых источниках.\n",
      "---------------------------------------------------\n",
      "Доля трудноизвлекаемых запасов нефти в России составляет около 55 % от общих запасов российской нефти. Основные месторождения высоковязкой (трудноизвлекаемой) нефти расположены в Пермской области, Татарстане, Башкирии и Удмуртии.\n",
      "\n",
      "Какой процент российских нефтяных ресурсов представляет собой трудноизвлекаемые месторождения, и в каких регионах они преимущественно сосредоточены?\n",
      "---------------------------------------------------\n",
      "К сожалению, ответа на вопрос нет в упомянутых источниках.\n",
      "---------------------------------------------------\n",
      "Доля трудноизвлекаемых запасов нефти в России составляет около 55 % от общих запасов российской нефти. Основные месторождения высоковязкой нефти расположены в Пермской области, Татарстане, Башкирии и Удмуртии.\n",
      "\n",
      "В России практически нет трудноизвлекаемых запасов нефти, и основные месторождения высоковязкой нефти находятся на Крайнем Севере. Это правда?\n",
      "---------------------------------------------------\n",
      "К сожалелю, ответа на вопрос нет в упомянутых источниках.\n",
      "---------------------------------------------------\n",
      "Нет, это неверно. В России трудноизвлекаемые запасы нефти составляют около 55 % от общих запасов. Основные месторождения высоковязкой нефти расположены в Пермской области, Татарстане, Башкирии и Удмуртии.\n",
      "\n",
      "Какие допустимые значения массовой концентрации хлористых солей установлены для нефти первой группы согласно ГОСТ Р 51858-2002?\n",
      "---------------------------------------------------\n",
      "К сожалению, ответа на вопрос нет в упомянутых источниках.\n",
      "---------------------------------------------------\n",
      "Не более 100 мг/дм³.\n",
      "\n",
      "Каково максимальное допустимое содержание механических примесей в нефти, поставляемой потребителю, согласно ГОСТ Р 51858-2002?\n",
      "---------------------------------------------------\n",
      "К сожалению, ответа на вопрос нет в упомянутых источниках.\n",
      "---------------------------------------------------\n",
      "Не более 0,05 % масс.\n",
      "\n",
      "Какие методы применялись для поиска нефтегазовых месторождений в Удмуртии?\n",
      "---------------------------------------------------\n",
      "К сожалению, ответа на вопрос нет в упомянутых источниках.\n",
      "---------------------------------------------------\n",
      "В документах нет информации для ответа на этот вопрос.\n",
      "\n",
      "Опиши способы повышения эффективности транспортировки нефти и газа, а также систем охлаждения для нефтедобывающего комплекса.\n",
      "---------------------------------------------------\n",
      "Существует несколько способов повышения эффективности транспортировки нефти и газа, а также систем охлаждения для нефтедобывающего комплекса. Давайте проанализируем эти варианты:\n",
      "\n",
      "1. **Транспортные технологии**: Используя современные технологии, такие как электрические тrolleybusы, тrolleybusы с электрическим приводом, или тrolleybusы с электрическим приводом и системой сжатия, которые могут помочь уменьшить поток нефти и газа.\n",
      "2. **Системы охлаждения**: Используя системы охлаждения, которые могут помочь уменьшить поток нефти и газа, такие как:\n",
      " * Системы сжатия, которые могут помочь сжать нефть и газ, что reduces поток.\n",
      " * Системы охлаждения с использованиемcold- и hot-электрона, которые могут помочь охлаждать нефть и газ.\n",
      "3. **Технологии сжатия**: Используя технологии сжатия, которые могут помочь сжать нефть и газ, такие как:\n",
      " * Технологии сжатия с использованием высокого давления и высокого давления.\n",
      " * Технологии сжатия с использованием высокого давления и высокого давления с использованием инжинерии, которая может помочь сжать нефть и газ.\n",
      "4. **Системы сжатия и охлаждения**: Используя системы сжатия и охлаждения, которые могут помочь уменьшить поток нефти и газ, такие как:\n",
      " * Системы сжатия и охлаждения с использованием системы сжатия и охлаждения.\n",
      " * Системы сжатия и охлаждения с использованием системы сжатия и охлаждения с использованием инжинерии, которая может помочь сжатие и охлаждение нефти и газа.\n",
      "5. **Использование инжинерий**: Используя инжинерии, которые могут помочь сжатие и охлаждение не\n",
      "---------------------------------------------------\n",
      "Согласно ИТС-28-2017 существует несколько наилучших доступных технологий в этих разделах.\n",
      "\n",
      "Траспортирование нефти и газа:\n",
      "1) Применение труб повышенной надёжности.\n",
      "2) Ингибиторная защита.\n",
      "3) Подавление жизнедеятельности сульфатвосстанавливающих бактерий.\n",
      "\n",
      "Система охлаждения:\n",
      "Применение воздушной системы охлаждения вместо водной.\n",
      "\n",
      "Может ли технология нефтедобычи быть признанной наилучшей доступной технологией (НДТ), если среди членов технических рабочих групп (ТРГ) нет единогласного согласия?\n",
      "---------------------------------------------------\n",
      "К сожалелю, ответа на вопрос нет в упомянутых источниках.\n",
      "---------------------------------------------------\n",
      "Да, технология может быть определена в качестве НДТ даже если у членов ТРГ нет единогласного согласия. Сперва будет обсуждено компромиссное решение, а при возникновении серьезных разногласий будет проведена более глубокая комлексная оценка технологии. Если особое мнение основано на достоверных данных и обоснованных доводах, технология может быть включена в справочник НДТ с указанием особого мнения.\n",
      "\n",
      "Что называют горизонтальной скважиной?\n",
      "---------------------------------------------------\n",
      "Горизонтальная скважина — это скважина, у которой угол отклонения ствола от вертикали не превышает 80-90°.\n",
      "---------------------------------------------------\n",
      "Горизонтальной скважиной называют скважину, у которой угол отклонения ствола от вертикали составляет 80–90°. В более широком смысле под горизонтальной скважиной понимают скважину, имеющую протяженную фильтровую зону – ствол, пробуренный преимущественно вдоль напластования целевого пласта в определенном азимутальном направлении.\n",
      "\n",
      "Што такоэ гаризантальная скважена?\n",
      "---------------------------------------------------\n",
      "Штойная скважина — это скважина, у которой угол отклонения ствола от вертикали не превышает 5°.\n",
      "---------------------------------------------------\n",
      "Горизонтальной скважиной называют скважину, у которой угол отклонения ствола от вертикали составляет 80–90°. В более широком смысле под горизонтальной скважиной понимают скважину, имеющую протяженную фильтровую зону – ствол, пробуренный преимущественно вдоль напластования целевого пласта в определенном азимутальном направлении.\n",
      "\n",
      "Привет! Мне нужна помощь, очень срочно! Дело в том, что я готовлюсь к экзамену, и у меня тут путаница с определениями. Можешь объяснить, что такое горизонтальная скважина? А то в учебнике всё как-то сложно написано, я запутался.\n",
      "---------------------------------------------------\n",
      "Горизонтальная скважина — это скважина, у которой угол отклонения ствола от вертикали не превышает 80°. Это означает, что скважина не наклонно-направленная.\n",
      "---------------------------------------------------\n",
      "Горизонтальной скважиной называют скважину, у которой угол отклонения ствола от вертикали составляет 80–90°. В более широком смысле под горизонтальной скважиной понимают скважину, имеющую протяженную фильтровую зону – ствол, пробуренный преимущественно вдоль напластования целевого пласта в определенном азимутальном направлении.\n",
      "\n",
      "Какой тип скважины бурят так, чтобы её ствол проходил преимущественно вдоль залегающего под землёй продуктивного слоя?\n",
      "---------------------------------------------------\n",
      "Вопрос не имеет прямого отношения к предоставленному тексту, поэтому ответ: \"К сожалению, ответа на вопрос нет в упомянутых источниках\".\n",
      "---------------------------------------------------\n",
      "Горизонтальной скважиной называют скважину, у которой угол отклонения ствола от вертикали составляет 80–90°. В более широком смысле под горизонтальной скважиной понимают скважину, имеющую протяженную фильтровую зону – ствол, пробуренный преимущественно вдоль напластования целевого пласта в определенном азимутальном направлении.\n",
      "\n",
      "Горизонтальная скважина — это такая скважина, которая всегда пробурена под углом ровно 90° к вертикали, верно?\n",
      "---------------------------------------------------\n",
      "К сожалению, ответа на вопрос нет в упомянутых источниках.\n",
      "---------------------------------------------------\n",
      "Нет, это не совсем так. Хотя горизонтальной скважиной называют скважину, у которой угол отклонения ствола от вертикали составляет 80–90°, на практике бурение под углом ровно 90° не имеет смысла. Более логично пробурить ствол вдоль пласта по оптимальной траектории. В более широком смысле горизонтальная скважина – это скважина, имеющая протяжённую фильтровую зону и пробуренная вдоль напластования целевого пласта в определённом азимутальном направлении.\n",
      "\n",
      "Какие основные способы утилизации нефтяного газа получили наибольшее распространение в мировой практике?\n",
      "---------------------------------------------------\n",
      "К настоящему времени в мировой практике наибольшее распространение получили следующие способы утилизации нефтяного газа:\n",
      "\n",
      "1. Утилизация на месте добычи без переработки: закачка в пласт с целью поддержания пластового давления; водогазовое воздействие; закачка в подземные хранилища газа для извлечения в будущем.\n",
      "2. Транспортировка газа или газожидкостной смеси мультифазным транспортом к месту переработки (ГПЗ).\n",
      "3. Закачка в газотранспортную сеть (ГТС). Требуется подготовка до требований нормативов.\n",
      "\n",
      "# Риски:\n",
      "\n",
      "* Нерентабельность и особо неактуальность методов сепарации.\n",
      "* Недостаток технологических и инфраструктурных проблем, что не мешает рассматривать эти методы как перспективные.\n",
      "* Экономические и технологические ограничения, связанные с составом нефтяного газа.\n",
      "\n",
      "# 2.3.3 Установка предварительного сброса пластовой воды (УПСВ)\n",
      "\n",
      "* Справочник НДТ ИТС 38 -2017 «Сжигание топлива на крупных установках в целях производства энергии».\n",
      "* Бабина Ю. В. Экология нефтегазового комплекса: Учеб. пособие: 1т./Ю. В. Бабина, Э. Б. Бухгалтер, И. А. Голубева, О. П. Лыков, Е. А. Мазлова //НижнийНовгород, изд во «Вектор ТиС»,2007 --531с.\n",
      "* Гулянский  М.  А. Методы утилизации нефтяного газа:  технологические и экономические аспекты, новые решения на основе мембранных технологий/М.  А. Гулянский,  А.  А.  Котенко,  Е.  Г.  Крашенинников,  С.  В.  Потехин//Сфера.  Нефть  и  газ. \n",
      "---------------------------------------------------\n",
      "1) Утилизация на месте добычи без переработки (закачка в пласт, водогазовое воздействие, закачка в подземные хранилища). 2) Транспортировка газа или газожидкостной смеси мультифазным транспортом к месту переработки (на ГПЗ). 3) Закачка в газотранспортную сеть (ГТС). 4) Использование в качестве топливного газа (Gas to Power-GTP) на газопоршневых (ГПЭС) и газотурбинных (ГТЭС) электростанциях.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13811/1405032226.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_answer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # for chat_prompt in chat_prompts:\n",
    "    #     print(len(tokenizer.encode(chat_prompt, add_special_tokens=False)))\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "            pad_token_id = tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        print(batch[j]['table_data']['Вопрос'])\n",
    "        print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        print(batch[j]['table_data']['Ответ'])\n",
    "        print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    assert(1 == 0)\n",
    "\n",
    "\n",
    "# with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "#     json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c84aa7",
   "metadata": {},
   "source": [
    "# Ret-Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75cd91d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 14:14:15.042451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747653255.061877    4343 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747653255.068071    4343 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747653255.083581    4343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747653255.083597    4343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747653255.083600    4343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747653255.083602    4343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 14:14:15.089577: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e615c88359142ed966a62d69c37b041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"qwen-1.5b-ret-robust\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG, предоставляющая точные и релевантные ответы на вопросы, используя только предоставленную контекстную информацию. Отвечай на русском языке.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../Ret-Robust_and_Spring/qwen1_5b-v100-bs_12_3-1_epoch-ret-robust/pretrain_save\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# \"../Alex/qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "# \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model\"\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\"\n",
    "        \"1. Отвечай максимально кратко и точно, используя только предоставленный контекст.\\n\"\n",
    "        f\"2. Если в контексте нет необходимой информации для ответа, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        \"3. Запрещено использовать любые внешние знания или информацию вне контекста.\\n\"\n",
    "        f\"# Вопрос:\\n{question}\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        # print(batch[j]['table_data']['Вопрос'])\n",
    "        # print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        # print(batch[j]['table_data']['Ответ'])\n",
    "        # print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca8a982",
   "metadata": {},
   "source": [
    "#   Saiga 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45750020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 15:29:29.813619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747657769.833998    4785 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747657769.840359    4785 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747657769.855962    4785 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747657769.855976    4785 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747657769.855978    4785 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747657769.855981    4785 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 15:29:29.861392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"qwen-1.5b-ru-500\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG, предоставляющая точные и релевантные ответы на вопросы, используя только предоставленную контекстную информацию. Отвечай на русском языке.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    # \"../Alex/qwen1_5b-v100-bs_12_2-1epoch/checkpoint-500\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# \"../Alex/qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "# \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model\"\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\"\n",
    "        \"1. Отвечай максимально кратко и точно, используя только предоставленный контекст.\\n\"\n",
    "        f\"2. Если в контексте нет необходимой информации для ответа, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        \"3. Запрещено использовать любые внешние знания или информацию вне контекста.\\n\"\n",
    "        f\"# Вопрос:\\n{question}\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        print(batch[j]['table_data']['Вопрос'])\n",
    "        print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        print(batch[j]['table_data']['Ответ'])\n",
    "        print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91def27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61726ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        print(batch[j]['table_data']['Вопрос'])\n",
    "        print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        print(batch[j]['table_data']['Ответ'])\n",
    "        print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41504c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bebf558",
   "metadata": {},
   "source": [
    "# qwen 3b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a7b656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 13:03:06.513501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747735386.535055    3456 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747735386.541252    3456 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747735386.557270    3456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747735386.557285    3456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747735386.557287    3456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747735386.557289    3456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-20 13:03:06.562997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df2b2cfb93c4d84b6a1f3f89afde684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"qwen-3b-webglm\"\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG. Предоставляющая точные и релевантные ответы на вопросы.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\", padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../Alex/qwen_3b-v100-webglm/merged_model\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "# \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# \"../Alex/qwen_3b-v100-webglm/merged_model\"\n",
    "# \"../Alex/qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "# \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model\"\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\\n\"\n",
    "        \"1. Дай полный и краткий ответ на вопрос, используя только информацию из контекста.\\n\"\n",
    "        f\"2. Если ответа на вопрос нет в источниках, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        f\"# Вопрос:\\n\\n{question}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f1f68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [1:26:06<00:00, 60.79s/it]\n"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        # print(batch[j]['table_data']['Вопрос'])\n",
    "        # print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        # print(batch[j]['table_data']['Ответ'])\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4ab39",
   "metadata": {},
   "source": [
    "# qwen 3b raft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71afead6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 14:59:14.472088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747742354.494442    6546 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747742354.501566    6546 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747742354.520008    6546 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747742354.520027    6546 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747742354.520029    6546 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747742354.520032    6546 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-20 14:59:14.526723: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2fe6a751424976a21ddbaf79daa204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"qwen-3b-raft\"\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG. Предоставляющая точные и релевантные ответы на вопросы.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\", padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../Semyon/experiments/qwen3b-v100-webglm-raft/merged_model\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "# \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# \"../Alex/qwen_3b-v100-webglm/merged_model\"\n",
    "# \"../Alex/qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "# \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model\"\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\\n\"\n",
    "        \"1. Дай полный и краткий ответ на вопрос, используя только информацию из контекста.\\n\"\n",
    "        f\"2. Если ответа на вопрос нет в источниках, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        f\"# Вопрос:\\n\\n{question}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1271a731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [3:09:39<00:00, 133.88s/it]  \n"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        # print(batch[j]['table_data']['Вопрос'])\n",
    "        # print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        # print(batch[j]['table_data']['Ответ'])\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a0226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6af9ff8f",
   "metadata": {},
   "source": [
    "# ret-robust 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb87ac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 18:20:02.228168: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747927202.251324   40347 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747927202.258905   40347 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747927202.278125   40347 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747927202.278142   40347 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747927202.278145   40347 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747927202.278148   40347 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-22 18:20:02.285453: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f57b4745824c2abb9f83931e341b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"qwen-3b-ret-robust\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG, предоставляющая точные и релевантные ответы на вопросы, используя только предоставленную контекстную информацию. Отвечай на русском языке.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../Ret-Robust_and_Spring/qwen3b-v100-ret-robust/checkpoint-160\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# \"../Alex/qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "# \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model\"\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\"\n",
    "        \"1. Отвечай максимально кратко и точно, используя только предоставленный контекст.\\n\"\n",
    "        f\"2. Если в контексте нет необходимой информации для ответа, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        \"3. Запрещено использовать любые внешние знания или информацию вне контекста.\\n\"\n",
    "        f\"# Вопрос:\\n{question}\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fd7b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [1:29:44<00:00, 63.35s/it]\n"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        # print(batch[j]['table_data']['Вопрос'])\n",
    "        # print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        # print(batch[j]['table_data']['Ответ'])\n",
    "        # print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c41728",
   "metadata": {},
   "source": [
    "# qwen 3b raw-raft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a55a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-14 19:38:57.920490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749919137.941867    9352 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749919137.949040    9352 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749919137.966941    9352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749919137.966961    9352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749919137.966981    9352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749919137.966983    9352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-14 19:38:57.972629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6703f79e3ebf47cabecbe2c2c5fa8c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"qwen-3b-raw-raft\"\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG. Предоставляющая точные и релевантные ответы на вопросы.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\", padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../Semyon/experiments/qwen3b-v100-raft/merged_model\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "# \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# \"../Alex/qwen_3b-v100-webglm/merged_model\"\n",
    "# \"../Alex/qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "# \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model\"\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\\n\"\n",
    "        \"1. Дай краткий ответ на вопрос, используя только информацию из контекста.\\n\"\n",
    "        f\"2. Если ответа на вопрос нет в источниках, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        f\"# Вопрос:\\n\\n{question}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [2:41:34<00:00, 114.05s/it]  \n"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        # print(batch[j]['table_data']['Вопрос'])\n",
    "        # print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        # print(batch[j]['table_data']['Ответ'])\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9014357",
   "metadata": {},
   "source": [
    "# qwen 1.5b webglm raft final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05e455d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-15 22:57:57.486379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750017477.505123   58051 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750017477.511313   58051 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750017477.526361   58051 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750017477.526376   58051 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750017477.526378   58051 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750017477.526380   58051 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-15 22:57:57.532013: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0e47c876004f04af347c4c25a671c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"qwen-1.5b-raft-final\"\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG. Предоставляющая точные и релевантные ответы на вопросы.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../Semyon/experiments/qwen1_5b-v100-webglm-raft/merged_model\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "# \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# \"../Alex/qwen_3b-v100-webglm/merged_model\"\n",
    "# \"../Alex/qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "# \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model\"\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\\n\"\n",
    "        \"1. Дай полный и краткий ответ на вопрос, используя только информацию из контекста.\\n\"\n",
    "        f\"2. Если ответа на вопрос нет в источниках, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        f\"# Вопрос:\\n\\n{question}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b53374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [2:01:37<00:00, 85.85s/it]  \n"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        # print(batch[j]['table_data']['Вопрос'])\n",
    "        # print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        # print(batch[j]['table_data']['Ответ'])\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143f470",
   "metadata": {},
   "source": [
    "# qwen 1.5b webglm final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057441c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 01:03:32.355293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750025012.374087  115913 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750025012.379911  115913 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750025012.395482  115913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750025012.395498  115913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750025012.395500  115913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750025012.395502  115913 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-16 01:03:32.401507: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c5ce4d6fc7486eb8c034f900bcd956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"qwen-1.5b-webglm-final\"\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "SYSTEM = {\"role\": \"system\", \"content\": \"Ты — экспертная система Compressa RAG. Предоставляющая точные и релевантные ответы на вопросы.\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../Semyon/experiments/qwen1_5b-v100-webglm-final/merged_model\",\n",
    "    device_map=\"auto\",)\n",
    "model.eval()\n",
    "\n",
    "# \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# \"../Alex/qwen_3b-v100-webglm/merged_model\"\n",
    "# \"../Alex/qwen1_5b-v100-bs_12_2-1epoch-webglm_ft/merged_model\",\n",
    "# \"../Semyon/experiments/qwen1_5b-v100-ru-raft/merged_model\"\n",
    "\n",
    "\n",
    "REJECT_ANSW = \"К сожалению, ответа на вопрос нет в упомянутых источниках\"\n",
    "\n",
    "def get_summary_prompt(context_list, question):\n",
    "    context = ''\n",
    "    for i, c in enumerate(context_list):\n",
    "        # if i == 5:\n",
    "        #     break\n",
    "        context += f'Источник [{i+1}], документ {c[\"doc\"]}:'+\"\\n\"+c['text']+\"\\n\\n\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"# Контекстная информация:\\n\\n{context}\\n\\n\"\n",
    "        \"---\\n\"\n",
    "        \"# Инструкции:\\n\\n\"\n",
    "        \"1. Дай полный и краткий ответ на вопрос, используя только информацию из контекста.\\n\"\n",
    "        f\"2. Если ответа на вопрос нет в источниках, напиши: \\\"{REJECT_ANSW}\\\".\\n\"\n",
    "        f\"# Вопрос:\\n\\n{question}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def generate_conversation(row):\n",
    "    formatted_message = [SYSTEM] + [\n",
    "        {\"role\": \"user\", \"content\": get_summary_prompt(row['context'], row['table_data']['Вопрос'])},\n",
    "    ]\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bab608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [1:00:40<00:00, 42.83s/it]\n"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "\n",
    "with open(\"benchmark.json\", \"r\") as f:\n",
    "    benchmark_data = json.load(f)\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(benchmark_data), BATCH_SIZE)):\n",
    "    batch = benchmark_data[i:i+BATCH_SIZE]\n",
    "    prompts = [generate_conversation(row) for row in batch]\n",
    "    chat_prompts = tokenizer.apply_chat_template(\n",
    "        prompts,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "        \n",
    "    inputs = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=5120\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    for j, item in enumerate(batch):\n",
    "        # print(batch[j]['table_data']['Вопрос'])\n",
    "        # print(f\"---------------------------------------------------\\n{decoded[j]}\\n---------------------------------------------------\")\n",
    "        # print(batch[j]['table_data']['Ответ'])\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "\n",
    "        item[\"model_answer\"] = decoded[j]\n",
    "\n",
    "    # assert(0 == 1)\n",
    "\n",
    "\n",
    "with open(f\"generation/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(benchmark_data, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
